{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates the performance of BERT models in varying classes to see if there are any class specific biases, and generates plots to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-processed dataset created in the fine-tuning notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2405\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19247\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2089\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "lm_datasets = DatasetDict()\n",
    "lm_datasets = lm_datasets.load_from_disk(\"processed_dataset\")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  102,\n",
       "  101,\n",
       "  1027,\n",
       "  7570,\n",
       "  7849,\n",
       "  2271,\n",
       "  13091,\n",
       "  7946,\n",
       "  1027,\n",
       "  102,\n",
       "  101,\n",
       "  102,\n",
       "  101,\n",
       "  7570,\n",
       "  7849,\n",
       "  2271,\n",
       "  13091,\n",
       "  7946,\n",
       "  1010,\n",
       "  2124,\n",
       "  2004,\n",
       "  1996,\n",
       "  2647,\n",
       "  27940,\n",
       "  2030,\n",
       "  2691,\n",
       "  27940,\n",
       "  1010,\n",
       "  2003,\n",
       "  1037,\n",
       "  2427,\n",
       "  1997,\n",
       "  1026,\n",
       "  4895,\n",
       "  2243,\n",
       "  1028,\n",
       "  27940,\n",
       "  2013,\n",
       "  1996,\n",
       "  2789,\n",
       "  4448,\n",
       "  4153,\n",
       "  1010,\n",
       "  7095,\n",
       "  2712,\n",
       "  1998,\n",
       "  3033,\n",
       "  1997,\n",
       "  1996,\n",
       "  2304,\n",
       "  2712,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  4876,\n",
       "  3141,\n",
       "  2000,\n",
       "  1996,\n",
       "  2137,\n",
       "  27940,\n",
       "  1010,\n",
       "  1044,\n",
       "  1012,\n",
       "  2137,\n",
       "  2271,\n",
       "  1012,\n",
       "  2009,\n",
       "  2089,\n",
       "  4982,\n",
       "  2000,\n",
       "  1037,\n",
       "  3091,\n",
       "  1997,\n",
       "  3438,\n",
       "  4642,\n",
       "  1006,\n",
       "  2484,\n",
       "  1999,\n",
       "  1007,\n",
       "  1998,\n",
       "  1037,\n",
       "  3742,\n",
       "  1997,\n",
       "  1020,\n",
       "  18857,\n",
       "  1006,\n",
       "  2410,\n",
       "  6053,\n",
       "  1007,\n",
       "  1010,\n",
       "  1998,\n",
       "  6468,\n",
       "  1037,\n",
       "  19194,\n",
       "  3940,\n",
       "  1997,\n",
       "  10702,\n",
       "  1012,\n",
       "  1999,\n",
       "  2166,\n",
       "  1010,\n",
       "  1996,\n",
       "  27940,\n",
       "  2015,\n",
       "  2024,\n",
       "  2630,\n",
       "  1010,\n",
       "  2069,\n",
       "  3352,\n",
       "  1000,\n",
       "  27940,\n",
       "  2417,\n",
       "  1000,\n",
       "  2006,\n",
       "  8434,\n",
       "  1012,\n",
       "  15100,\n",
       "  5158,\n",
       "  1999,\n",
       "  1996,\n",
       "  2621,\n",
       "  1010,\n",
       "  5155,\n",
       "  6763,\n",
       "  2029,\n",
       "  2024,\n",
       "  3344],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [101,\n",
       "  102,\n",
       "  101,\n",
       "  1027,\n",
       "  7570,\n",
       "  7849,\n",
       "  2271,\n",
       "  13091,\n",
       "  7946,\n",
       "  1027,\n",
       "  102,\n",
       "  101,\n",
       "  102,\n",
       "  101,\n",
       "  7570,\n",
       "  7849,\n",
       "  2271,\n",
       "  13091,\n",
       "  7946,\n",
       "  1010,\n",
       "  2124,\n",
       "  2004,\n",
       "  1996,\n",
       "  2647,\n",
       "  27940,\n",
       "  2030,\n",
       "  2691,\n",
       "  27940,\n",
       "  1010,\n",
       "  2003,\n",
       "  1037,\n",
       "  2427,\n",
       "  1997,\n",
       "  1026,\n",
       "  4895,\n",
       "  2243,\n",
       "  1028,\n",
       "  27940,\n",
       "  2013,\n",
       "  1996,\n",
       "  2789,\n",
       "  4448,\n",
       "  4153,\n",
       "  1010,\n",
       "  7095,\n",
       "  2712,\n",
       "  1998,\n",
       "  3033,\n",
       "  1997,\n",
       "  1996,\n",
       "  2304,\n",
       "  2712,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  4876,\n",
       "  3141,\n",
       "  2000,\n",
       "  1996,\n",
       "  2137,\n",
       "  27940,\n",
       "  1010,\n",
       "  1044,\n",
       "  1012,\n",
       "  2137,\n",
       "  2271,\n",
       "  1012,\n",
       "  2009,\n",
       "  2089,\n",
       "  4982,\n",
       "  2000,\n",
       "  1037,\n",
       "  3091,\n",
       "  1997,\n",
       "  3438,\n",
       "  4642,\n",
       "  1006,\n",
       "  2484,\n",
       "  1999,\n",
       "  1007,\n",
       "  1998,\n",
       "  1037,\n",
       "  3742,\n",
       "  1997,\n",
       "  1020,\n",
       "  18857,\n",
       "  1006,\n",
       "  2410,\n",
       "  6053,\n",
       "  1007,\n",
       "  1010,\n",
       "  1998,\n",
       "  6468,\n",
       "  1037,\n",
       "  19194,\n",
       "  3940,\n",
       "  1997,\n",
       "  10702,\n",
       "  1012,\n",
       "  1999,\n",
       "  2166,\n",
       "  1010,\n",
       "  1996,\n",
       "  27940,\n",
       "  2015,\n",
       "  2024,\n",
       "  2630,\n",
       "  1010,\n",
       "  2069,\n",
       "  3352,\n",
       "  1000,\n",
       "  27940,\n",
       "  2417,\n",
       "  1000,\n",
       "  2006,\n",
       "  8434,\n",
       "  1012,\n",
       "  15100,\n",
       "  5158,\n",
       "  1999,\n",
       "  1996,\n",
       "  2621,\n",
       "  1010,\n",
       "  5155,\n",
       "  6763,\n",
       "  2029,\n",
       "  2024,\n",
       "  3344]}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets['validation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom class based on the HuggingFace trainer is used to perform class-specific evaluations for all frequently occurring words.  The evaluation results on each of the fine-tuned models are saved as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "from collections import defaultdict\n",
    "\n",
    "class ClassEvaluator(Trainer):\n",
    "    \n",
    "    def eval_by_class(self, num_occurrences=10):\n",
    "        \n",
    "        model = self._wrap_model(self.model, training=False)\n",
    "        model.eval()\n",
    "        loss_by_class = defaultdict(list)\n",
    "        mean_loss_by_class = defaultdict(float)\n",
    "        eval_dataloader = self.get_eval_dataloader()\n",
    "        #eval_dataloader = DataLoader(lm_datasets[\"validation\"], shuffle=False)\n",
    "        prediction_loss_only = False\n",
    "        \n",
    "        for step, inputs in enumerate(eval_dataloader):\n",
    "            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=[])\n",
    "            labels = tokenizer.batch_decode(labels[0])\n",
    "            loss = float(loss.cpu().numpy())\n",
    "            for label in labels:\n",
    "                if label != -100:\n",
    "                    loss_by_class[label].append(loss)\n",
    "                    \n",
    "        for key, val in loss_by_class.items():\n",
    "            if len(val) >= num_occurrences:\n",
    "                mean_loss_by_class[key] = np.mean(val)\n",
    "        \n",
    "        return mean_loss_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file weight_decay_0.1\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file weight_decay_0.1\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at weight_decay_0.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file weight_decay_0.001\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file weight_decay_0.001\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay_0.1\n",
      "defaultdict(<class 'float'>, {'[ U N K ]': 1.3516051642478704, 't h e': 1.3350357412659397, 'i s': 1.2479027586774185, 'o f': 1.289786870955858, 't o': 1.3777351341806017, 'a': 1.3214487993834614, ',': 1.3319983650006324, '# # s': 1.2462144109510607, 'w h i c h': 1.4780005116177641, 'f o r': 1.3652947747987279, 'i n t o': 1.4087561403329556, '=': 1.2118638854840105, '# # k': 1.2591075489314034, '(': 1.1553287502617864, ')': 1.2319662949982906, '–': 1.2652182544653232, '.': 1.3065580959140748, '@': 1.1969425614520455, '>': 1.2412914283848657, 'i n': 1.3349970583659803, '<': 1.2310448790848962, 'w i t h': 1.3272188122044066, 'a s': 1.3536375921100514, '\"': 1.3866898571715502, 'u n': 1.2433113140149687, 'o n l y': 1.406488505101973, 't h i s': 1.4285267083083881, 'b y': 1.357469740073073, 'b e': 1.4256658112009366, 'a r e': 1.3360017138448628, '1': 1.3518323920391224, '-': 1.2151804627293812, 't h r e e': 1.2010040791595684, 'a n d': 1.332562365137395, 't h e i r': 1.427156243084082, 'a l s o': 1.3279647973545812, 'f r o m': 1.481885628545121, 'w e s t': 1.4663600531908183, 'o n e': 1.4288082375364788, ':': 1.3908593576578867, 't h a n': 1.7113650012016297, 'o n': 1.3531421005948265, 'i t': 1.4636941144086477, 'w h e r e': 1.4946754143155854, 'o r': 1.4215467870235443, 'b e e n': 1.463284723325209, 'u n t i l': 1.520181470903857, 'w a s': 1.3677244719616708, 'h e': 1.3376826420118493, 'f o r c e s': 1.4762454581047808, 'd u r i n g': 1.4284023516650857, 'a u s t r a l i a n': 1.386506274724618, 'e a r l y': 1.493177764415741, 'p a r t': 1.3749042056702279, 'o v e r': 1.522511398792267, 'w a r': 1.327353012177252, 'a t': 1.4067896947163303, 'w h e n': 1.4087511992288961, 'i t s': 1.3408074544704691, 'w e r e': 1.472446653880735, 'w h i l e': 1.4179854086216759, 'n o': 1.3763556116157107, 's': 1.3599183072910663, 'h a d': 1.373543555730755, '2': 1.53952916264534, 't h e y': 1.4937729324195899, 'a l l': 1.5453774318579705, \"'\": 1.2808411721314978, 'n o r t h': 1.3247968714411666, 'a f t e r': 1.4026196170598269, 'o t h e r': 1.4158664732365995, 'b r i t i s h': 1.4245517528974092, 'b e t w e e n': 1.1713440732564777, 'f i r s t': 1.4609043816936778, 't w o': 1.335874017144813, 'a n': 1.3402651490135626, 'm': 1.2673736485915306, 'a l o n g': 1.2506129410531786, 'c i t y': 1.252287801275862, 'n e w': 1.3072468287414976, 't i m e': 1.5840436673164369, 'h i s': 1.388348080559608, 't h a t': 1.5573337442916015, 'f o u r': 1.5528058329453835, 'w h o': 1.3322279560379684, 'h a s': 1.3495553548519428, 'u n i t e d': 0.992417645725337, 'a l t h o u g h': 1.4930122273939628, 'b u t': 1.4521657496450409, ';': 1.4747159533541312, 'm o s t': 1.2640514017215796, 's e v e r a l': 1.269076539624122, 'b a c k': 1.5104578813681235, 'h i m': 1.3064287777604728, 'd a y': 1.5904897832870484, '—': 1.6434248840367351, 's t a t e': 1.4379683037598927, 'u n d e r': 1.4808385328010276, 'a b o u t': 1.4034251216295603, 'm o r e': 1.3279112988349162, 's o u t h': 1.3240859934756921, '0 0 0': 1.08524316035468, 'h a v e': 1.4400630999118724, 'u s': 1.6065764987107478, 'i': 1.347355329138892, 's h e': 1.3506381727362935, 'h e r': 1.3554153185751703, 't h e r e': 1.238357750698924, 's o m e': 1.660262861559468, 'u p': 1.406288942805043, 's o n g': 1.34298574924469, 'n o t': 1.4787500535066311, 'a g a i n s t': 1.2116026251480496, 's e p t e m b e r': 1.2304482570400945, 'l a t e r': 1.3299625664949417, 'w o u l d': 1.4811485158279538, 'b': 1.4024879158689425, 'r o u t e': 1.1426067426800728, 's c i e n t o l o g y': 1.572635439152901})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at weight_decay_0.001.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file weight_decay_0.01\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file weight_decay_0.01\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay_0.001\n",
      "defaultdict(<class 'float'>, {'[ U N K ]': 1.361698891422439, 't h e': 1.3439193029902123, 'i s': 1.2599050842225552, 'o f': 1.2990581869750684, 't o': 1.3837576337471917, 'a': 1.3275881874712279, ',': 1.3410191340266606, '# # s': 1.2453731610890357, 'w h i c h': 1.489036724295305, 'f o r': 1.3760644091673977, 'i n t o': 1.4270782654102032, '=': 1.2333741884062188, '# # k': 1.2690102644254035, '(': 1.1665797115069383, ')': 1.2492097415180015, '–': 1.2712051146305525, '.': 1.315097163113865, '@': 1.208021377540445, '>': 1.2497617102393443, 'i n': 1.347207340485796, '<': 1.239947945741335, 'w i t h': 1.3381741225362538, 'a s': 1.3574354181653017, '\"': 1.3915439794986482, 'u n': 1.251974939273547, 'o n l y': 1.4218370270344518, 't h i s': 1.4461632826749016, 'b y': 1.363544518457089, 'b e': 1.4239104678233465, 'a r e': 1.3513503281907602, '1': 1.3725884148368128, '-': 1.2352933385914153, 't h r e e': 1.2096314272459816, 'a n d': 1.341016850824901, 't h e i r': 1.4351091954245496, 'a l s o': 1.3268139728328638, 'f r o m': 1.490331576295095, 'w e s t': 1.4942656709597661, 'o n e': 1.450522583925118, ':': 1.4084138845403988, 't h a n': 1.7129111194610596, 'o n': 1.3572772672372078, 'i t': 1.470213847106877, 'w h e r e': 1.5081402926609433, 'o r': 1.4313603478509027, 'b e e n': 1.452179369059476, 'u n t i l': 1.5215355414768745, 'w a s': 1.3760148156275096, 'h e': 1.3459922720885966, 'f o r c e s': 1.4905240764575345, 'd u r i n g': 1.4341371658033337, 'a u s t r a l i a n': 1.390853077937395, 'e a r l y': 1.4760867977142333, 'p a r t': 1.380597467358048, 'o v e r': 1.5322502149002892, 'w a r': 1.3317012142750524, 'a t': 1.4104668404780278, 'w h e n': 1.4370068907737732, 'i t s': 1.3385498278907366, 'w e r e': 1.4770580306093195, 'w h i l e': 1.43015455410761, 'n o': 1.3797351221243541, 's': 1.3657259913201028, 'h a d': 1.3795552762966712, '2': 1.543339737256368, 't h e y': 1.503908256397528, 'a l l': 1.5455879261416774, \"'\": 1.2900078111203783, 'n o r t h': 1.3285578947241714, 'a f t e r': 1.4066923495847732, 'o t h e r': 1.4210710702715694, 'b r i t i s h': 1.4327401587596307, 'b e t w e e n': 1.1849048032891005, 'f i r s t': 1.4620196332639837, 't w o': 1.3416958226532232, 'a n': 1.3458757851611485, 'm': 1.280806424908149, 'a l o n g': 1.2897624825989757, 'c i t y': 1.2631237050320239, 'n e w': 1.3239376491970487, 't i m e': 1.5904448837041856, 'h i s': 1.3989471704314609, 't h a t': 1.5708614209015381, 'f o u r': 1.5931749767982042, 'w h o': 1.3622563569677166, 'h a s': 1.3612762414492094, 'u n i t e d': 0.9838455838687492, 'a l t h o u g h': 1.5071719167409119, 'b u t': 1.449386578053236, ';': 1.4872931633709054, 'm o s t': 1.2667568145053727, 's e v e r a l': 1.2574497230591313, 'b a c k': 1.5111476320486803, 'h i m': 1.3318511972139622, 'd a y': 1.6023624444007873, '—': 1.6921845398567341, 's t a t e': 1.4285575876633325, 'u n d e r': 1.4835208698555276, 'a b o u t': 1.4077704182347737, 'm o r e': 1.321033307097175, 's o u t h': 1.3216974024855814, '0 0 0': 1.0865301757023251, 'h a v e': 1.4376461413946557, 'u s': 1.6388112017602632, 'i': 1.3704505801200866, 's h e': 1.3433726739726568, 'h e r': 1.3518218733370304, 't h e r e': 1.243806965649128, 's o m e': 1.6756116959356493, 'u p': 1.4112090931998358, 's o n g': 1.337608779852207, 'n o t': 1.4890434123002565, 'a g a i n s t': 1.2380429064405376, 's e p t e m b e r': 1.2333547633003306, 'l a t e r': 1.3347548746293592, 'w o u l d': 1.4898975305259228, 'b': 1.4188676522328303, 'r o u t e': 1.187296912074089, 's c i e n t o l o g y': 1.5575009704782412})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at weight_decay_0.01.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file weight_decay_0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file weight_decay_0\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay_0.01\n",
      "defaultdict(<class 'float'>, {'[ U N K ]': 1.3619171611051084, 't h e': 1.3445668791947591, 'i s': 1.2582521087991503, 'o f': 1.299620762967738, 't o': 1.3836991051631469, 'a': 1.3270847381601079, ',': 1.3408960130855434, '# # s': 1.2436082738061105, 'w h i c h': 1.488353762127783, 'f o r': 1.3752925527688549, 'i n t o': 1.4302231657963533, '=': 1.2323902739341452, '# # k': 1.2683486245902023, '(': 1.1682825962528152, ')': 1.2475120055695508, '–': 1.2690206981049135, '.': 1.3151183898430674, '@': 1.208009678004795, '>': 1.2501003294628061, 'i n': 1.3467572666955347, '<': 1.2408821569229331, 'w i t h': 1.338246851267215, 'a s': 1.3565518732042345, '\"': 1.3934315084553082, 'u n': 1.2519466066508274, 'o n l y': 1.4208924597309482, 't h i s': 1.4451245757586815, 'b y': 1.3661377654156686, 'b e': 1.4277395009994507, 'a r e': 1.3473705387250943, '1': 1.3722651335928175, '-': 1.2326740927723971, 't h r e e': 1.2103593463406843, 'a n d': 1.3416461884788358, 't h e i r': 1.436896136447565, 'a l s o': 1.3331605750217772, 'f r o m': 1.4917225045700595, 'w e s t': 1.4989015001517076, 'o n e': 1.4470488403813313, ':': 1.4036281534603663, 't h a n': 1.7178887128829956, 'o n': 1.3573284917580548, 'i t': 1.4668434021411507, 'w h e r e': 1.5116535137439597, 'o r': 1.4296236779238727, 'b e e n': 1.4537868413058195, 'u n t i l': 1.5265633587179512, 'w a s': 1.3754010466895892, 'h e': 1.3514892119329331, 'f o r c e s': 1.4917163359267371, 'd u r i n g': 1.4348528403146514, 'a u s t r a l i a n': 1.3905762540988433, 'e a r l y': 1.478609435558319, 'p a r t': 1.3824263835275494, 'o v e r': 1.536069114293371, 'w a r': 1.3350428189000776, 'a t': 1.4146302612054915, 'w h e n': 1.4436149729622736, 'i t s': 1.3399011919997177, 'w e r e': 1.4754218310673735, 'w h i l e': 1.4307815826991026, 'n o': 1.3853092149451927, 's': 1.3669453642152725, 'h a d': 1.3798085150209445, '2': 1.5461475243171057, 't h e y': 1.5030719859927308, 'a l l': 1.5518722774520997, \"'\": 1.2901132110681128, 'n o r t h': 1.3279645900900772, 'a f t e r': 1.4072316114325076, 'o t h e r': 1.4204209140829138, 'b r i t i s h': 1.435502929183153, 'b e t w e e n': 1.1855110046453774, 'f i r s t': 1.4636438274954229, 't w o': 1.3405465908714982, 'a n': 1.3498196970332752, 'm': 1.2834231685369442, 'a l o n g': 1.2921731957682856, 'c i t y': 1.2596888206106551, 'n e w': 1.3224055588245391, 't i m e': 1.598545434474945, 'h i s': 1.397772653523935, 't h a t': 1.5664389308890685, 'f o u r': 1.593205112677354, 'w h o': 1.3557382792674213, 'h a s': 1.3597687138960912, 'u n i t e d': 0.9857736871098027, 'a l t h o u g h': 1.5095825890700023, 'b u t': 1.446665516183261, ';': 1.487391195909092, 'm o s t': 1.2691996289151055, 's e v e r a l': 1.26527847589985, 'b a c k': 1.5098260515011275, 'h i m': 1.3302353625667507, 'd a y': 1.6123247694969178, '—': 1.682345617700506, 's t a t e': 1.437757323185603, 'u n d e r': 1.4859391804094668, 'a b o u t': 1.4074155362071217, 'm o r e': 1.319587357116468, 's o u t h': 1.3181836819925974, '0 0 0': 1.090917990639292, 'h a v e': 1.4377657000688797, 'u s': 1.636648099530827, 'i': 1.37327966434615, 's h e': 1.3498884827682847, 'h e r': 1.3513377213643656, 't h e r e': 1.2483973614871502, 's o m e': 1.6770005408794648, 'u p': 1.4158670869138505, 's o n g': 1.346963264621221, 'n o t': 1.4923241853713989, 'a g a i n s t': 1.2341484108875538, 's e p t e m b e r': 1.2303057152915884, 'l a t e r': 1.3315850818349468, 'w o u l d': 1.477290072478354, 'b': 1.4163584714898696, 'r o u t e': 1.1775017695294485, 's c i e n t o l o g y': 1.5523155125287862})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at weight_decay_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file dropout_0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file dropout_0\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay_0\n",
      "defaultdict(<class 'float'>, {'[ U N K ]': 1.361166267224901, 't h e': 1.3432300448753904, 'i s': 1.262110506155743, 'o f': 1.2986413913252222, 't o': 1.3837623681324918, 'a': 1.325890183630513, ',': 1.340380396028248, '# # s': 1.2438869365761358, 'w h i c h': 1.486194876872975, 'f o r': 1.3759665765403384, 'i n t o': 1.4250052399360216, '=': 1.234349058713201, '# # k': 1.2682272301566733, '(': 1.1667544866601627, ')': 1.249343843768107, '–': 1.2702355178502889, '.': 1.3152577669275918, '@': 1.2061942259408782, '>': 1.2494562363703061, 'i n': 1.3470735886656902, '<': 1.2402488742263793, 'w i t h': 1.3379817443515012, 'a s': 1.354592662049483, '\"': 1.3884523059889244, 'u n': 1.2516326764593262, 'o n l y': 1.4198246127174747, 't h i s': 1.4441661019535625, 'b y': 1.3646016504012086, 'b e': 1.4339067757129669, 'a r e': 1.3473732432181185, '1': 1.3743027525919456, '-': 1.2329161239208302, 't h r e e': 1.209541067481041, 'a n d': 1.3400863160950303, 't h e i r': 1.4361633965328557, 'a l s o': 1.33189885030713, 'f r o m': 1.4898534494720093, 'w e s t': 1.50030383009177, 'o n e': 1.4478061456801528, ':': 1.4067219208393777, 't h a n': 1.711497733592987, 'o n': 1.358618132992467, 'i t': 1.4683833666543924, 'w h e r e': 1.5078162653692837, 'o r': 1.4271741445000108, 'b e e n': 1.4575040638446808, 'u n t i l': 1.5259894492297337, 'w a s': 1.3759294732810765, 'h e': 1.3447440937161446, 'f o r c e s': 1.495372328375067, 'd u r i n g': 1.4332565714059204, 'a u s t r a l i a n': 1.3836476099796784, 'e a r l y': 1.4753147196769714, 'p a r t': 1.3749572016500138, 'o v e r': 1.5366193260465348, 'w a r': 1.3372575794496844, 'a t': 1.4113470258153216, 'w h e n': 1.4349258173671033, 'i t s': 1.337934738215135, 'w e r e': 1.478560068714085, 'w h i l e': 1.4338326278854818, 'n o': 1.386179816943628, 's': 1.3687687572012557, 'h a d': 1.378666940823342, '2': 1.535817606250445, 't h e y': 1.503061644879042, 'a l l': 1.5462997219254893, \"'\": 1.2889554880371163, 'n o r t h': 1.3253379856667868, 'a f t e r': 1.4031637522857636, 'o t h e r': 1.4195842678482469, 'b r i t i s h': 1.4289122143617043, 'b e t w e e n': 1.1870192382484674, 'f i r s t': 1.4631173317419721, 't w o': 1.3395285420730465, 'a n': 1.3507660452615131, 'm': 1.279772804524654, 'a l o n g': 1.2895467733895336, 'c i t y': 1.2615506756812969, 'n e w': 1.328429338004854, 't i m e': 1.5891205996274949, 'h i s': 1.3995335892252965, 't h a t': 1.566545961158616, 'f o u r': 1.59879154081528, 'w h o': 1.3501707424099247, 'h a s': 1.3579218357037275, 'u n i t e d': 0.9799247406648867, 'a l t h o u g h': 1.509063265941761, 'b u t': 1.4481596565775332, ';': 1.4796248877590352, 'm o s t': 1.2619880022747176, 's e v e r a l': 1.2700433019668824, 'b a c k': 1.5064645753456996, 'h i m': 1.3298228198084339, 'd a y': 1.6127543258666992, '—': 1.6827306195541665, 's t a t e': 1.4419965296983719, 'u n d e r': 1.4864473475350275, 'a b o u t': 1.4061770455257312, 'm o r e': 1.3159774541854858, 's o u t h': 1.3198381499495617, '0 0 0': 1.089040035831517, 'h a v e': 1.4432022894950622, 'u s': 1.6370001796520117, 'i': 1.3626313763005393, 's h e': 1.3400507667347004, 'h e r': 1.3561589125957754, 't h e r e': 1.2509485883638263, 's o m e': 1.6720317534862026, 'u p': 1.414067088453858, 's o n g': 1.3441233485937119, 'n o t': 1.4902847008063242, 'a g a i n s t': 1.2282853064865902, 's e p t e m b e r': 1.2406438015125416, 'l a t e r': 1.3368027825509348, 'w o u l d': 1.477661358192563, 'b': 1.4145672355706875, 'r o u t e': 1.1763525381684303, 's c i e n t o l o g y': 1.5578653084544034})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at dropout_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file dropout_0.2\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.2,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file dropout_0.2\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_0\n",
      "defaultdict(<class 'float'>, {'[ U N K ]': 1.3680177421610606, 't h e': 1.3493399762534357, 'i s': 1.2627351451235322, 'o f': 1.3066695575140113, 't o': 1.3948871415030133, 'a': 1.3401073467084268, ',': 1.3502906646313582, '# # s': 1.2480623871088028, 'w h i c h': 1.4858750328421593, 'f o r': 1.3819429198105033, 'i n t o': 1.3934102218884687, '=': 1.2246909219441082, '# # k': 1.2777154727411921, '(': 1.1701006667132963, ')': 1.2559996144283538, '–': 1.2795370860168567, '.': 1.3174284453299834, '@': 1.2078660633489688, '>': 1.2612397732674274, 'i n': 1.3520872595533227, '<': 1.2479807192475532, 'w i t h': 1.3357972920297863, 'a s': 1.3842362403597461, '\"': 1.411400669755468, 'u n': 1.2584371374309582, 'o n l y': 1.4505291033175685, 't h i s': 1.4488533625707907, 'b y': 1.37960789749893, 'b e': 1.4474230262140433, 'a r e': 1.3755498626354066, '1': 1.4081049064795177, '-': 1.2329325373577593, 't h r e e': 1.2365280959536047, 'a n d': 1.3442234679736864, 't h e i r': 1.443842719517537, 'a l s o': 1.3516945551361954, 'f r o m': 1.5014911873699868, 'w e s t': 1.4674296940748508, 'o n e': 1.4734496570239632, ':': 1.4089315363339014, 't h a n': 1.7453824424743651, 'o n': 1.3630992313151928, 'i t': 1.4590252861949324, 'w h e r e': 1.53677286053526, 'o r': 1.4188369786417163, 'b e e n': 1.47433395114812, 'u n t i l': 1.5209215838333656, 'w a s': 1.3768441207558342, 'h e': 1.3493964604356072, 'f o r c e s': 1.4932982719370298, 'd u r i n g': 1.4395355160894066, 'a u s t r a l i a n': 1.4341900944709778, 'e a r l y': 1.5217744779586793, 'p a r t': 1.4120294749736786, 'o v e r': 1.5656618535518647, 'w a r': 1.3573782742023468, 'a t': 1.435457102802335, 'w h e n': 1.399754839638869, 'i t s': 1.3585164981837174, 'w e r e': 1.5002251003882778, 'w h i l e': 1.4323740750551224, 'n o': 1.3787205980883703, 's': 1.3786986841800366, 'h a d': 1.368313733614909, '2': 1.5402381817499797, 't h e y': 1.511892383589464, 'a l l': 1.539800874167873, \"'\": 1.295581961232187, 'n o r t h': 1.3343890098536886, 'a f t e r': 1.4189200156833977, 'o t h e r': 1.429916360088297, 'b r i t i s h': 1.4514441203612547, 'b e t w e e n': 1.1729729110375047, 'f i r s t': 1.4953211485705478, 't w o': 1.3393554404133656, 'a n': 1.3648810164494949, 'm': 1.247844832065778, 'a l o n g': 1.272980855570899, 'c i t y': 1.260832853773807, 'n e w': 1.315737783577707, 't i m e': 1.5964647352695465, 'h i s': 1.3879518312051755, 't h a t': 1.5760137401337695, 'f o u r': 1.5466639800713613, 'w h o': 1.3422325845911272, 'h a s': 1.3492436791077638, 'u n i t e d': 0.9905963409127612, 'a l t h o u g h': 1.4907796217335596, 'b u t': 1.4451532950324397, ';': 1.521537308665839, 'm o s t': 1.2574627266398497, 's e v e r a l': 1.268004527976436, 'b a c k': 1.5305218283946698, 'h i m': 1.3103505254819476, 'd a y': 1.6204698371887207, '—': 1.66417893215462, 's t a t e': 1.4210886498292288, 'u n d e r': 1.4915321226473208, 'a b o u t': 1.446952602347812, 'm o r e': 1.3751996434999234, 's o u t h': 1.365559122243593, '0 0 0': 1.1140694315063542, 'h a v e': 1.4618167553810364, 'u s': 1.5818537040190264, 'i': 1.3694273378167834, 's h e': 1.3806374943570088, 'h e r': 1.3591346426142588, 't h e r e': 1.2594839558005333, 's o m e': 1.645530439192249, 'u p': 1.4491885315488886, 's o n g': 1.344268248631404, 'n o t': 1.4863531701839887, 'a g a i n s t': 1.2361532716915524, 's e p t e m b e r': 1.3132514931537487, 'l a t e r': 1.3600823119763406, 'w o u l d': 1.478691608645022, 'b': 1.4136276485828252, 'r o u t e': 1.1206811848613951, 's c i e n t o l o g y': 1.6084538772702217})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at dropout_0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file dropout_0.4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.4,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.4,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file dropout_0.4\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_0.2\n",
      "defaultdict(<class 'float'>, {'[ U N K ]': 1.3795302285655098, 't h e': 1.36493179098544, 'i s': 1.2814308321819856, 'o f': 1.3158218929151235, 't o': 1.3944324671707533, 'a': 1.344276746577718, ',': 1.3601727729044153, '# # s': 1.2145741221404844, 'w h i c h': 1.49787874183739, 'f o r': 1.4060826220482752, 'i n t o': 1.4447371329252536, '=': 1.2478550170197618, '# # k': 1.2869558891342259, '(': 1.1805702364453448, ')': 1.2473563443644335, '–': 1.281726654045857, '.': 1.3361863830660798, '@': 1.2281942899011444, '>': 1.2650923387886581, 'i n': 1.3662811062508042, '<': 1.2532763652500851, 'w i t h': 1.3591718577339265, 'a s': 1.3757970693040655, '\"': 1.4114565715716854, 'u n': 1.2667183257925807, 'o n l y': 1.4243262342868312, 't h i s': 1.4465719759464264, 'b y': 1.3721364328271843, 'b e': 1.463110277056694, 'a r e': 1.3721901501444254, '1': 1.379117164346907, '-': 1.2526893857964216, 't h r e e': 1.215263718191315, 'a n d': 1.3692076691971924, 't h e i r': 1.4617284064862266, 'a l s o': 1.359011939220261, 'f r o m': 1.5098051637002867, 'w e s t': 1.5514851556374476, 'o n e': 1.4767651704408356, ':': 1.4399944643179576, 't h a n': 1.7384166789054871, 'o n': 1.3741421092357209, 'i t': 1.5098003437955871, 'w h e r e': 1.554923812890875, 'o r': 1.4706486044703304, 'b e e n': 1.4631558700041338, 'u n t i l': 1.5156599755944877, 'w a s': 1.3993085332193607, 'h e': 1.3686352278575424, 'f o r c e s': 1.5138930204723562, 'd u r i n g': 1.4314248415416684, 'a u s t r a l i a n': 1.418367122992491, 'e a r l y': 1.50394522190094, 'p a r t': 1.4033767701806248, 'o v e r': 1.5527235542024884, 'w a r': 1.375346653884457, 'a t': 1.4305096478486548, 'w h e n': 1.4690067263113127, 'i t s': 1.3605320458205379, 'w e r e': 1.4842275471718454, 'w h i l e': 1.4480908565661486, 'n o': 1.4167727254055165, 's': 1.3953384210771702, 'h a d': 1.396047339856046, '2': 1.5416118115186692, 't h e y': 1.5436198565305448, 'a l l': 1.541476713072869, \"'\": 1.2983918594001433, 'n o r t h': 1.3504636411259814, 'a f t e r': 1.435845747590065, 'o t h e r': 1.434624470568992, 'b r i t i s h': 1.4373425852793913, 'b e t w e e n': 1.1782486182637513, 'f i r s t': 1.4808639043823202, 't w o': 1.353155122428644, 'a n': 1.3458758312192831, 'm': 1.2990252746221347, 'a l o n g': 1.3020977896672707, 'c i t y': 1.2808604494054268, 'n e w': 1.36799686021275, 't i m e': 1.5955725365877151, 'h i s': 1.4082385334399863, 't h a t': 1.5942895173145633, 'f o u r': 1.6064999516193683, 'w h o': 1.4049313560697354, 'h a s': 1.3931745053865972, 'u n i t e d': 1.0174772436871673, 'a l t h o u g h': 1.4933525765383686, 'b u t': 1.4949406435893429, ';': 1.4762415681316545, 'm o s t': 1.259644593511309, 's e v e r a l': 1.2845018957891772, 'b a c k': 1.5761844962835312, 'h i m': 1.3314861388042056, 'd a y': 1.6278517293930053, '—': 1.6802999410364363, 's t a t e': 1.4327960123618444, 'u n d e r': 1.5254358605102256, 'a b o u t': 1.4213967709927946, 'm o r e': 1.3145048518975575, 's o u t h': 1.3504520297743554, '0 0 0': 1.1148766875267029, 'h a v e': 1.4516221627275994, 'u s': 1.6299425228075548, 'i': 1.3783854961395263, 's h e': 1.3576617699704672, 'h e r': 1.3713736567232344, 't h e r e': 1.2636518133804202, 's o m e': 1.7249664798859627, 'u p': 1.4018967195793435, 's o n g': 1.3586997802440937, 'n o t': 1.5237373054027556, 'a g a i n s t': 1.2163769911075462, 's e p t e m b e r': 1.27535586003904, 'l a t e r': 1.3642470711661923, 'w o u l d': 1.5148232793435454, 'b': 1.4186338358200514, 'r o u t e': 1.1395039409399033, 's c i e n t o l o g y': 1.5590403469709249})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at dropout_0.4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_0.4\n",
      "defaultdict(<class 'float'>, {'[ U N K ]': 1.530184575982787, 't h e': 1.519358749959994, 'i s': 1.4622400550171732, 'o f': 1.460686225295536, 't o': 1.5518941091983258, 'a': 1.4840270846131596, ',': 1.51661851382568, '# # s': 1.3869184767526965, 'w h i c h': 1.681631602873297, 'f o r': 1.5855629768490134, 'i n t o': 1.6050457713695674, '=': 1.3992393344855136, '# # k': 1.4204857538183153, '(': 1.3104594792321063, ')': 1.3931078644806907, '–': 1.4579674171713681, '.': 1.4920696105760496, '@': 1.3779467549003825, '>': 1.4074564646462777, 'i n': 1.5132378724580418, '<': 1.3888622870939196, 'w i t h': 1.5177354590889223, 'a s': 1.4908995538881924, '\"': 1.572257203967225, 'u n': 1.4088975514666, 'o n l y': 1.570563284620162, 't h i s': 1.5995631651843296, 'b y': 1.5189728927418438, 'b e': 1.6056731204191843, 'a r e': 1.4968201448971574, '1': 1.5303485459751553, '-': 1.3949096801111123, 't h r e e': 1.3604488960083794, 'a n d': 1.5349219628535582, 't h e i r': 1.615402096243047, 'a l s o': 1.5185702390838087, 'f r o m': 1.6626750267531774, 'w e s t': 1.7128543326487908, 'o n e': 1.6620674133300781, ':': 1.617560047478903, 't h a n': 1.9444177770614623, 'o n': 1.5186159900987326, 'i t': 1.6955467921518188, 'w h e r e': 1.699953453294162, 'o r': 1.6057505237089622, 'b e e n': 1.6415965459563515, 'u n t i l': 1.6862346930750485, 'w a s': 1.5594951499179217, 'h e': 1.5109753196766553, 'f o r c e s': 1.677477246948651, 'd u r i n g': 1.5598262902991524, 'a u s t r a l i a n': 1.5353131217834277, 'e a r l y': 1.6349941873550415, 'p a r t': 1.585202096281825, 'o v e r': 1.7307604474680764, 'w a r': 1.5316268763234537, 'a t': 1.5706232481059574, 'w h e n': 1.6512296083900664, 'i t s': 1.4710773561073809, 'w e r e': 1.6542276692256999, 'w h i l e': 1.6257535631165785, 'n o': 1.580031872899444, 's': 1.547519498809855, 'h a d': 1.5485080333010663, '2': 1.697693516810735, 't h e y': 1.6951719458196677, 'a l l': 1.6475997874813695, \"'\": 1.440678166541597, 'n o r t h': 1.5475131745745496, 'a f t e r': 1.587353433482349, 'o t h e r': 1.6286697178273588, 'b r i t i s h': 1.6126953615592077, 'b e t w e e n': 1.392345161177218, 'f i r s t': 1.6532972306013107, 't w o': 1.4605674533570399, 'a n': 1.4759125007824465, 'm': 1.4790483033045745, 'a l o n g': 1.4836821015234347, 'c i t y': 1.4875069691779765, 'n e w': 1.5288077566358778, 't i m e': 1.7761954867839813, 'h i s': 1.5694199281001309, 't h a t': 1.73247028160565, 'f o u r': 1.743427696136328, 'w h o': 1.49671034598335, 'h a s': 1.6025958840663617, 'u n i t e d': 1.1351393763766144, 'a l t h o u g h': 1.7033792447160792, 'b u t': 1.637035433563494, ';': 1.6353659017971067, 'm o s t': 1.4276362998144967, 's e v e r a l': 1.4242494481225167, 'b a c k': 1.705188825726509, 'h i m': 1.5445246244299, 'd a y': 1.7517066407203674, '—': 1.9822925483738933, 's t a t e': 1.6252344747384389, 'u n d e r': 1.6798915598127577, 'a b o u t': 1.5607081764453166, 'm o r e': 1.444446805751685, 's o u t h': 1.5233794280955957, '0 0 0': 1.2808574314775139, 'h a v e': 1.6666637151799304, 'u s': 1.781782455516584, 'i': 1.5010923189776284, 's h e': 1.5001646241075115, 'h e r': 1.549925745361381, 't h e r e': 1.4188525583595037, 's o m e': 1.9215316387914843, 'u p': 1.504004463001534, 's o n g': 1.499676675750659, 'n o t': 1.6802361607551575, 'a g a i n s t': 1.3433982051652054, 's e p t e m b e r': 1.4274519008618813, 'l a t e r': 1.5091807404833455, 'w o u l d': 1.7043969109654427, 'b': 1.4614252287607927, 'r o u t e': 1.3011064761214786, 's c i e n t o l o g y': 1.75114611020455})\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"weight_decay_0.1\", \"weight_decay_0.001\", \"weight_decay_0.01\", \"weight_decay_0\", \"dropout_0\", \"dropout_0.2\", \"dropout_0.4\"]\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    model = model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        per_device_eval_batch_size = 1,\n",
    "        output_dir = \"eval_tmp\"\n",
    "    )\n",
    "\n",
    "    class_evaluator = ClassEvaluator(\n",
    "        model = model,\n",
    "        args = eval_args,\n",
    "        train_dataset = lm_datasets[\"train\"],\n",
    "        eval_dataset = lm_datasets[\"validation\"],\n",
    "        data_collator = data_collator\n",
    "    )\n",
    "\n",
    "    class_probs = class_evaluator.eval_by_class(25)\n",
    "    \n",
    "    print(model_name)\n",
    "    print(class_probs)\n",
    "    np.save(model_name + \"_dict.npy\", class_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words/classes with a high difference in perplexity across different weight decay and dropout settings and over 25 occurrences in the validation set are graphed.  These images are used in the final paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.04598570786989642, 'f o u r'), (-0.03930573551743133, '—'), (-0.038933832336355056, 'a l o n g'), (-0.03394377690095163, 'w e s t'), (-0.033745795488357544, 'r o u t e'), (-0.03042368094126391, 'u s'), (-0.026174618138207206, 'w h e n'), (-0.023394042047961028, 'h i m'), (-0.022485173229190636, '='), (-0.022470360552823143, '1'), (-0.022264542579650826, 'd a y'), (-0.02118250926335641, 'n e w'), (-0.019126870270286256, 'f o r c e s'), (-0.01899790814367397, 'o n e'), (-0.017942786371956343, 'w h o'), (-0.01773566119144898, '-'), (-0.017377548769816453, ')'), (-0.01668268133854056, 'a g a i n s t'), (-0.016249099603065975, 'i n t o'), (-0.016051038547798324, 'a l t h o u g h'), (-0.01586256318149104, ':'), (-0.015847219263805945, 'w h i l e'), (-0.015675164991989732, 'b e t w e e n'), (-0.015639393645174415, 't h i s'), (-0.015276047161647321, 'i'), (-0.014207747478324428, 'i s'), (-0.014107927254267905, 'o v e r'), (-0.013336107615501636, 'o n l y'), (-0.013140851053698333, 'w h e r e'), (-0.01259083766490221, 't h e r e'), (-0.012399155933123351, 'm'), (-0.012079319701745028, 'b'), (-0.012076530299709898, 'i n'), (-0.011768891926734693, 's o m e'), (-0.011534647299693113, 'n o t'), (-0.011425736398376296, '('), (-0.011371529373255695, 'a r e'), (-0.01118550866568846, 'h i s'), (-0.010762932147094562, 'w i t h'), (-0.010671801741610532, 'f o r'), (-0.010500896247950475, 'a n'), (-0.010195544472447082, 's e p t e m b e r'), (-0.009904567272432274, 'w a r'), (-0.009824205327917168, 'n o'), (-0.009561102977030611, '[ U N K ]'), (-0.009288712459452064, 't h e y'), (-0.009262874405434829, 'c i t y'), (-0.009251664488832745, '@'), (-0.009212216867014567, 't h a t'), (-0.009203995141483068, '<'), (-0.009119681225269849, '# # k'), (-0.009007153448773675, 't h e i r'), (-0.008854520369364183, 'o f'), (-0.00885044991018935, 's'), (-0.008699671013516985, '.'), (-0.008536988321472538, 't h r e e'), (-0.0083820310276157, ','), (-0.008366480851784708, 'h a s'), (-0.008321362444357439, 'u n'), (-0.008240964512030269, 'b e'), (-0.008205001319405714, 'w a s'), (-0.008194365255210778, 'w h i c h'), (-0.008194303609450726, 't h e'), (-0.008164807985440392, '>'), (-0.008114315905618508, \"'\"), (-0.007967820926888303, 'f r o m'), (-0.007778145648815071, 'u p'), (-0.00752395095763525, 'a n d'), (-0.007131910328135538, 'b y'), (-0.007061451704295285, 'h e'), (-0.006840216055993054, 'l a t e r'), (-0.006113414833349928, 'w e r e'), (-0.0060272339518900875, 't o'), (-0.00580797832587665, 'u n t i l'), (-0.005627357476466477, 'o r'), (-0.005608814733999878, 'u n d e r'), (-0.005476032397640385, 'o n'), (-0.0051233850925871405, 'h a d'), (-0.005076932311057991, 't i m e'), (-0.0050172633849656645, '–'), (-0.004908934404904031, ';'), (-0.004854219740834775, 'd u r i n g'), (-0.0046892522457446795, 'i t'), (-0.0045573310989912574, 'a t'), (-0.00444138424705165, 'a'), (-0.00436046146429514, 'b r i t i s h'), (-0.00402822593847918, 's t a t e'), (-0.003934052952548894, 'a l s o'), (-0.003796875476837158, '0 0 0'), (-0.0037177946116473404, 'o t h e r'), (-0.0036545249282335934, 't w o'), (-0.0031391895831898164, 'h a v e'), (-0.002751923896170849, 'a b o u t'), (-0.00221295004829436, 'f i r s t'), (-0.001762448817374196, '\"'), (-0.0011375993490219116, 's o n g'), (-0.0009667623427604966, 's e v e r a l'), (-0.0009550699394316187, 'a s'), (-0.0009222900675187606, 'a l l'), (-0.0007435940206050873, 'h e r'), (-0.0005441352259367704, 'a f t e r'), (-0.0005411142256201984, 'n o r t h'), (-0.0001327323913573597, 't h a n'), (-5.299597978591919e-05, 'p a r t'), (0.0020633994468619576, 'm o s t'), (0.002327474374924865, '# # s'), (0.0028586647449395652, 'a u s t r a l i a n'), (0.00287271625533414, 'i t s'), (0.0034871576353907585, 'w o u l d'), (0.003711556394895066, '2'), (0.003993306022423981, 'b a c k'), (0.0040060930675076545, 'b u t'), (0.00424784352613039, 's o u t h'), (0.005780659480528305, 'b e e n'), (0.010587406001593136, 's h e'), (0.01193384464943037, 'm o r e'), (0.01249290506045031, 'u n i t e d'), (0.01477013069849753, 's c i e n t o l o g y'), (0.01786304473876954, 'e a r l y')]\n"
     ]
    }
   ],
   "source": [
    "dict_1 = np.load(\"weight_decay_0_dict.npy\", allow_pickle=True).item()\n",
    "dict_2 = np.load(\"weight_decay_0.1_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "l = []\n",
    "for key, _ in dict_1.items():\n",
    "    diff = dict_2[key] - dict_1[key]\n",
    "    l.append((diff, key))\n",
    "    \n",
    "l.sort()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.060694791602365905, 'm o r e'), (-0.04941353029929685, 's c i e n t o l o g y'), (-0.04729181196954513, 'u p'), (-0.0452957405341845, ';'), (-0.03789563311470867, 's e p t e m b e r'), (-0.03348826496831836, '# # s'), (-0.028987742132610794, '1'), (-0.026202869030737208, 'o n l y'), (-0.02555583135501749, 'a b o u t'), (-0.022975724386541607, 's h e'), (-0.02126437776228962, 't h r e e'), (-0.019776280584006267, 'a g a i n s t'), (-0.019005185230211774, 'a n'), (-0.01782925605773933, 'e a r l y'), (-0.01599755321643248, 'w e r e'), (-0.01582297147848677, 'a u s t r a l i a n'), (-0.015107092469237582, 's o u t h'), (-0.01445724418822758, 'f i r s t'), (-0.014101535081863403, 'b r i t i s h'), (-0.012938299349376292, 'o v e r'), (-0.011178081143986107, 'b e e n'), (-0.010194592653437029, 'h a v e'), (-0.0086527047930538, 'p a r t'), (-0.008643270063920294, ')'), (-0.008439171055680594, 'a s'), (-0.008110674547738173, 'd u r i n g'), (-0.007471464671745753, 'b y'), (-0.006965763568878014, 't h a n'), (-0.005261608238877891, 'u n t i l'), (-0.004947454953680319, 'a t'), (-0.003359712490981215, 'a r e'), (-0.002281386624364279, 't h i s'), (-0.0008921986818313243, 't i m e'), (-0.00045467433225998555, 't o'), (5.5901816217396316e-05, '\"'), (0.0008072560203487011, '0 0 0'), (0.0013736297686894883, '2'), (0.0016758389049960076, 'a l l'), (0.002015547636820436, 'i t s'), (0.0021818668714592704, 'm o s t'), (0.0021895680290002772, '–'), (0.0025729548048090134, 'a l t h o u g h'), (0.002809898167956293, \"'\"), (0.0033155134168723066, 'o n e'), (0.0038525655212306997, '>'), (0.004164759189851708, 'l a t e r'), (0.004167857579886913, 't h e r e'), (0.004169399869291102, 'a'), (0.004708110480694971, 'o t h e r'), (0.005006187237226234, 'b'), (0.005275707226246595, 'b e t w e e n'), (0.005295646002531962, '<'), (0.007317384084065681, 'a l s o'), (0.007381892204284535, 'd a y'), (0.008281188361622505, 'u n'), (0.008313976330299955, 'f r o m'), (0.008958158322742937, 'i'), (0.009152335401112266, 'o f'), (0.009240416393033746, '# # k'), (0.009882108273057133, ','), (0.01046956973204849, '('), (0.01104287792052805, 'o n'), (0.01151248640444913, '[ U N K ]'), (0.011707362532615617, 's t a t e'), (0.012003708995230733, 'w h i c h'), (0.012239014108975654, 'h e r'), (0.01379968201527837, 't w o'), (0.014193846697481582, 'i n'), (0.01443153161268973, 's o n g'), (0.015591814732004217, 't h e'), (0.015687250842650657, 'b e'), (0.01571678151102618, 'w h i l e'), (0.016074631272292805, 'n o r t h'), (0.016121008881816223, '—'), (0.016497367812741137, 's e v e r a l'), (0.016639736897133606, 's'), (0.01692573190666735, 'a f t e r'), (0.01788568696868964, 't h e i r'), (0.01796837968211018, 'w a r'), (0.018150952355614924, 'w h e r e'), (0.018275777180793762, 't h a t'), (0.01869568705845337, 'i s'), (0.018757937736096464, '.'), (0.01882275607850814, 'r o u t e'), (0.0192387674219352, 'h e'), (0.019756848438662322, '-'), (0.02002759563161982, 'c i t y'), (0.02028670223481077, 'h i s'), (0.020328226552175588, '@'), (0.02059474853532639, 'f o r c e s'), (0.021135613322257996, 'h i m'), (0.022464412463526484, 'w a s'), (0.023164095075653535, '='), (0.023374565704140204, 'w i t h'), (0.024139702237771843, 'f o r'), (0.024984201223505975, 'a n d'), (0.026880902774406135, 'u n i t e d'), (0.027733606241137077, 'h a d'), (0.029116934096371727, 'a l o n g'), (0.031062927984056188, ':'), (0.0317274729410808, 't h e y'), (0.03390373786290479, 'u n d e r'), (0.03613167069852352, 'w o u l d'), (0.037384135218766934, 'n o t'), (0.038052127317146134, 'n o'), (0.04393082627883338, 'h a s'), (0.04566266788886142, 'b a c k'), (0.04808881878852844, 'u s'), (0.049787348556903144, 'b u t'), (0.050775057600654705, 'i t'), (0.051180442556356676, 'm'), (0.051326911036784884, 'i n t o'), (0.051811625828614094, 'o r'), (0.05225907663504281, 'n e w'), (0.059835971548007016, 'f o u r'), (0.06269877147860825, 'w h o'), (0.06925188667244364, 'w h e n'), (0.0794360406937138, 's o m e'), (0.0840554615625968, 'w e s t')]\n"
     ]
    }
   ],
   "source": [
    "dict_1 = np.load(\"dropout_0_dict.npy\", allow_pickle=True).item()\n",
    "dict_2 = np.load(\"dropout_0.2_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "l = []\n",
    "for key, _ in dict_1.items():\n",
    "    diff = dict_2[key] - dict_1[key]\n",
    "    l.append((diff, key))\n",
    "    \n",
    "l.sort()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file weight_decay_0.1\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file weight_decay_0.1\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at weight_decay_0.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2089' max='2089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2089/2089 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file weight_decay_0.001\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file weight_decay_0.001\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay_0.1\n",
      "loss {'eval_loss': 1.353858470916748, 'eval_runtime': 30.9131, 'eval_samples_per_second': 67.577, 'eval_steps_per_second': 67.577}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at weight_decay_0.001.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2089' max='2089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2089/2089 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file weight_decay_0.01\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file weight_decay_0.01\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay_0.001\n",
      "loss {'eval_loss': 1.3638619184494019, 'eval_runtime': 31.1924, 'eval_samples_per_second': 66.972, 'eval_steps_per_second': 66.972}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at weight_decay_0.01.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2089' max='2089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2089/2089 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file weight_decay_0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file weight_decay_0\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay_0.01\n",
      "loss {'eval_loss': 1.3640527725219727, 'eval_runtime': 30.1467, 'eval_samples_per_second': 69.294, 'eval_steps_per_second': 69.294}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at weight_decay_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2089' max='2089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2089/2089 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file dropout_0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file dropout_0\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay_0\n",
      "loss {'eval_loss': 1.363321304321289, 'eval_runtime': 29.9475, 'eval_samples_per_second': 69.755, 'eval_steps_per_second': 69.755}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at dropout_0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2089' max='2089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2089/2089 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file dropout_0.2\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.2,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file dropout_0.2\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_0\n",
      "loss {'eval_loss': 1.3702541589736938, 'eval_runtime': 30.1877, 'eval_samples_per_second': 69.2, 'eval_steps_per_second': 69.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at dropout_0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2089' max='2089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2089/2089 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file dropout_0.4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.4,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.4,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file dropout_0.4\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_0.2\n",
      "loss {'eval_loss': 1.381805658340454, 'eval_runtime': 30.0444, 'eval_samples_per_second': 69.53, 'eval_steps_per_second': 69.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at dropout_0.4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2089' max='2089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2089/2089 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_0.4\n",
      "loss {'eval_loss': 1.5328611135482788, 'eval_runtime': 30.0904, 'eval_samples_per_second': 69.424, 'eval_steps_per_second': 69.424}\n"
     ]
    }
   ],
   "source": [
    "losses = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    model = model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    \n",
    "    eval_args = TrainingArguments(\n",
    "        per_device_eval_batch_size = 1,\n",
    "        output_dir = \"eval_tmp\"\n",
    "    )\n",
    "\n",
    "    class_evaluator = ClassEvaluator(\n",
    "        model = model,\n",
    "        args = eval_args,\n",
    "        train_dataset = lm_datasets[\"train\"],\n",
    "        eval_dataset = lm_datasets[\"validation\"],\n",
    "        data_collator = data_collator\n",
    "    )\n",
    "    eval_results = class_evaluator.evaluate()\n",
    "    print(model_name)\n",
    "    print(\"loss \" + str(eval_results))\n",
    "    losses[model_name] = np.exp(eval_results[\"eval_loss\"])\n",
    "    \n",
    "np.save(\"model_perplexities.npy\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAApJUlEQVR4nO3de3xU1bn/8c+TSSCBcBOioECjvyoqd4x4v1LBitfWih7bc7y89GeriD0WqZdjU1tP9WiPR6un1l6A9lgrIlUrVo62iD8VhYQ7CNZalZsaUCJggCTz/P7Yk2QymSSTMJPb/r5fzGtmr7323s/sCeuZtfeetc3dERGR8Mpq7wBERKR9KRGIiIScEoGISMgpEYiIhJwSgYhIyCkRiIiEXHZ7B9BSAwYM8MLCwvYOQ0SkUyktLd3m7gXJ5nW6RFBYWEhJSUl7hyEi0qmY2QeNzdOhIRGRkFMiEBEJOSUCEZGQUyIQEQk5JQIRYf5785k4dyKjZo9i4tyJzH9vfnuHJG0oFIlAf+Qtp30WHvPfm0/xG8Vs3b0Vx9m6eyvFbxTrMw+RTnf5aEvV/JHvqd4DUPtHDjD5sMltHo+743jdM07wr6486tGgbkK9miHD3Z0o0dp5ydYLEPVo/bKE7SR9xlm0cRGPrnqUfdX7gGCf3fnGnWzeuZmTB59c//2QMIx5g8n6BYnDnjdYPoVlWlq/NTE02EZz62zpNprZT61aZ+LyKe6He5fcW/v/o8ae6j08UPoAX/nSV+ge6d7kdqTzs852P4KioiJvye8IJs6dyNbdWxuURyzCQT0Oqt+gJmkoofUNamPrE+lMcrJy6NWtV/DI6UV+t/xmp3t3611bnp+TT5aF4uBDh2Zmpe5elGxel+8RfLT7o6Tl1V5N0cAiDMPM6j0Dta9r/oCbqpdFVl2Zxeomq2dZtfOS1WvwHHvd2PaT1ouLO/G9pBr3zYtubnR/PnjGg7XrqFGz7drpZuYnSqyf0joTl2kw2cIYU4ihwfz93A/JttlcDM3F3ew2kuyn6/9yPdsqtjXYdp9ufbhixBXs3LeTnft2smvfLj6v/Jxd+3bxyRefsGvfLnZW7qSiqqLZ95Gfk1+XMHKCpJHfLb/2de10wvze3XrTq1sv9UoyrMsngoE9BybtEQzqOYi7T767HSLq+AaVDGp0n5059Mx2iEgy6XtF36t3+BQgN5LLrcfdmtLh08poJbv27aqXKGqSx859O9lV2XD6ky8+4e87/s7OyiDBVHt1k9tQrySzunwimDZuWtI/8mnjprVjVB2b9lm41DT2Dy57kI92f8TAngOZNm5ayufQcrJy6Jfbj365/Vq1fXenoqqiLllU7qzXC0mcbsteSfyjK/dKuvw5AghOGLf2jzystM+kM2lNryQ+8XT0Xkk6/j82dY4gFIlARKQpib2SXZW7+HxfXEJppFcS32tJd6+kd06QREo/LuWRFY+wt3pv7XpyI7kUn1jcomQQ6pPFIiLNMTN65PSgR04PDup5UKvW0RbnSmrsqd7Dg8seTFsvXYlARCQN0n2upKZXcv1frk9av7ErIltDiUBEpANorFcyqGfyq/gG9hyYtm3reioRkQ5s2rhp5EZy65Wl+yo+9QhERDqw/b28NxVKBCIiHdzkwyZn9PJtHRoSEQk5JQIRkZBTIhARCbmMJQIzyzWzJWa20szWmtkPk9QZamYLzWy5ma0ys3MyFY+IiCSXyR7BXuBMdx8NjAHONrPjE+rcAcxx97HApcB/ZzAeERFJImNXDXkwiNGu2GRO7JE4sJEDvWOv+wBbMhWPiIgkl9FzBGYWMbMVwCfAS+7+VkKVYuCbZrYJeAGY2sh6rjWzEjMrKSsry2TIIiKhk9FE4O7V7j4GGAyMN7MRCVUuA2a5+2DgHOB3Zg3HaXX3x9y9yN2LCgoKMhmyiEjotMlVQ+6+A1gInJ0w62pgTqzOYiAXGNAWMYmISCCTVw0VmFnf2Os84CxgfUK1D4EJsTpHESQCHfsREWlDmRxiYhAw28wiBAlnjrs/b2Z3ASXu/hxwM/BLM/suwYnjK7yz3SlHRKSTy+RVQ6uAsUnK74x7vQ44KVMxiIhI8/TLYhGRkFMiEBEJOSUCEZGQUyIQEQk5JQIRkZBTIhARCTklAhGRkFMiEBEJOSUCEZGQUyIQEQk5JQIRkZBTIhARCTklAhGRkFMiEBEJOSUCEZGQUyIQEQk5JQIRgVVz4IERUNw3eF41p70jkjaUyVtVikhnsGoO/OlGqKwIpss3BtMAoy5pv7ikzYQjEayaA3+5C8o3QZ/BMOFO/YE3R/us/UWj4NUQra7/7N6wLFoNHq17rjcvYbq2Xmzei7fWJYEalRXB56/PPBS6fiLQt52WS2WfNdZIRaNJGqKExiexIUpWt8l50TQ1hMnWk674G1t3su0mmfbq9vns45VvhLf/BEOOg/wD2zsaySBz9/aOoUWKioq8pKQk9QUeGBH8QSfKyoEBR8Qm4vZB7f5ooqzePmttmTeYlfk4Ulz/nvL65fEsK2ioOiWDrAhYJO45K2464XW9ullJ6sfPS6yfrnVHwJLFnY7txsqe/CfY9UnTu65fYZAQhowPng88OliHdBpmVuruRcnmdf0eQfmm5OXRSjjg0Lpps7iZlkJZ3LzWlqW8TZqol4FtLvkFjTr5X/ejYUyh8UrWSKat0bTG31eYTby7fg8QICcPzvkp9P8ybHwrePx9Iax6MpjfrRcMPqYuORxSBHl92yV82X9dPxH0GZy8R9BnCFz6eNvH0xlseKHxfTbh39o+HsmsmsN9jZ0TGnpc8OwOOz6AjUvqksOr98V6iAYHHlXXYxhyHBxwmJJvJ5GxQ0Nmlgu8CnQnSDhz3f0HSepdAhQTHItY6e7/1NR6W3xoKPF4NwTfds57SOcIGqN9JqnauxM2l8Ylh6WwtzyY16N//cNJB48N/o6kXbTXoaG9wJnuvsvMcoDXzOzP7v5mXGCHA7cCJ7n7Z2aW/jNSzX3bkYa0zyRV3XvBYacHDwhOiG/bEEsKseSw4YVgXlY2DBpdlxwGj4c+h7RX5BKnTU4Wm1kP4DXg2+7+Vlz5fwDvuPuvUl1Xi3sEItK+dm+DTUvrksPmUqjaE8zrPTjucNJ4GDgSIjntG28X1W4ni80sApQCXwYeiU8CMUfE6r0ORIBid38xyXquBa4FGDp0aCZDFpF06zkAhn01eABU7YOPV9f1GD58E9bOC+Zl58Ehx9RPDj0OaL/YQ6KtegR9gT8CU919TVz580AlcAkwmOCcwkh339HYutQjEOmCyjfVP5y0dVXdbyn6H17/XMOAI4KrxqRF2v3yUXffYWYLgbOBNXGzNgFvuXsl8A8zewc4HFjaFnGJSAfRZ3DwGPH1YHrfF7BleV1y2PACrPifYF5un+D8Qu2lq8dA9/z2i70LyFgiMLMCoDKWBPKAs4B7E6o9A1wGzDSzAQSHit7LVEwi0kl06wGFJwUPCC5d3f73ustWNy6Bd18K5lkWHDSi7rLVIeOh71BdutoCmewRDAJmx84TZAFz3P15M7sLKHH354AFwEQzWwdUA9PdfXsGYxKRzsgMBnw5eIy9PCir+Aw2ldYlhxW/h6W/DOblD6z/m4ZBoyC7e/vF38F1/SEmRCQcqqvgk3X1zzXs+CCYF+kOB4+pnxxCNn5SU+cIlAhEpOva+VHcj92WwNYVUL0vmBey8ZPa/WSxiEi76DUQjj4/eABU7YWtKxsZPykfBheFcvwkJQIRCY/s7rEewHhgqsZPilEiEJHwMgsOEfUrrBtCJXH8pDV/hNJZwbwuOn6SEoGISLyWjp80cFT95NAJx0/SyWIRkZbqhOMn6WSxiEg6NTd+0sa3OtX4SeoRiIhkQvmmWGKIJYePVkG0KpjXDuMnqUcgItLWasdP+lowvT/jJ62ak9H7gygRiIi0hdaOn+QeJIyacxDlG4M7CELakoEODYmIdBSJ4ydtLoV9u5LX7TMEvrsm+bwkdGhIRKQzyOsHh38leEAwftKPBhDc0j1B+aa0bVZ3dxAR6agi2cE5gWQaK28FJQIRkY5swp0Nf72ckxeUp4kSgYhIRzbqEjjvoeCcABY8n/eQrhoSEQmVUZekteFPpB6BiEjIKRGIiIScEoGISMjpHIGIZExlZSWbNm1iz5497R1KaOTm5jJ48GByclIf8VSJQEQyZtOmTfTq1YvCwkKsC97Zq6Nxd7Zv386mTZs49NBDU14upUNDZta/1ZGJSGjt2bOH/v37Kwm0ETOjf//+Le6BpXqO4E0ze8rMzrEUP1EzyzWzJWa20szWmtkPm6j7dTNzM0s6DoaIdF5KAm2rNfs71URwBPAY8C3gb2b272Z2RDPL7AXOdPfRwBjgbDM7PrGSmfUCpgFvpRy1iIikTUqJwAMvuftlwDXAvwBLzGyRmZ3QxDI1w+blxB7Jhjr9EXAvoLNJIpJ2hYWFvP/++5x++umtXsejjz7Kb3/7WwBmzZrFli1bWrT8+++/z4gRIwB45ZVXuOKKK5g1axbFxcWtjimdUjpZHDtH8E2CHsHHwFTgOYJv+k8BSc9KmFkEKAW+DDzi7m8lzB8HDHH3+WY2vZXvQUS6iGeWb+a+BRvYsqOCg/vmMX3SMC4c2/43g7/uuutqX8+aNYsRI0Zw8MEHt2NE6ZXqoaHFQG/gQnef7O7z3L3K3UuARxtbyN2r3X0MMBgYb2YjauaZWRbwn8DNzW3czK41sxIzKykrK0sxZBHpTJ5Zvplb561m844KHNi8o4Jb563mmeWb92u9BQUFRCIRDjgguE/wrFmzuOGGG2rnn3vuubzyyisA5Ofnc/vttzN69GiOP/54Pv74YwCKi4u5//77mTt3LiUlJVx++eWMGTOGiooKSktLOe200zjmmGOYNGkSW7duBaC0tJTRo0czevRoHnnkkdrtdevWjT59+pCXl0d+ftxdyNpRqpeP3uHuc+ILzOwb7v6Uu9/b3MLuvsPMFgJnAzV3UugFjABeiZ3cGAg8Z2bnxxJM/PKPEZyjoKioqHPdSUdEAPjhn9aybsvnjc5f/uEO9lVH65VVVFZzy9xVPLHkw6TLHH1wb35w3vAmt7t06VIA5s2b12yMu3fv5vjjj+fuu+/mlltu4Ze//CV33HFH7fyLL76Yhx9+mPvvv5+ioiIqKyuZOnUqzz77LAUFBTz55JPcfvvt/OY3v+HKK6/k4Ycf5tRTT2X69LoDHieeeCInnnhis7G0pVR7BN9PUnZrUwuYWYGZ9Y29zgPOAtbXzHf3cncf4O6F7l4IvAk0SAIiEg6JSaC58kzo1q0b5557LgDHHHMM77//fpP1N2zYwJo1azjrrLMYM2YMP/7xj9m0aRM7duxgx44dnHrqqQB861vfynTo+6XJHoGZfRU4BzjEzB6Km9UbqGpm3YOA2bHzBFnAHHd/3szuAkrc/bn9iFtEOpnmvrmfdM9f2byjokH5IX3zePL/Jr0mpVWys7OJRuuSS/w19zk5ObWXX0YiEaqqmm7m3J3hw4ezePHieuU7duxIW7xtobkewRaghOCKntK4x3PApKYWdPdV7j7W3Ue5+wh3vytWfmeyJODup6s3IBJe0ycNIy8nUq8sLyfC9EnD0rqdwsJCVqxYQTQaZePGjSxZsqRFy/fq1YudO3cCMGzYMMrKymoTQWVlJWvXrqVv37707duX1157DYDHH388re8h3ZrsEbj7SmClmT3u7s31AEREWq3m6qBMXzV00kknceihh3L00Udz1FFHMW7cuBYtf8UVV3DdddeRl5fH4sWLmTt3LjfeeCPl5eVUVVVx0003MXz4cGbOnMlVV12FmTFx4sS0vod0M/fGz72a2Rx3v8TMVpPkNwDuPiqTwSVTVFTkJSXqOIh0Bm+//TZHHXVUe4cROsn2u5mVunvS0Ruau2poWuz53DTEJiIiHVBzh4a2xl72dPd18fPM7HTgg8yEJSIibSXVy0fnmNkMC+SZ2c+An2QyMBERaRupJoLjgCHAG8BSgquJTspUUCIi0nZSTQSVQAWQB+QC/3D3tvuVh4iIZEyqiWApQSI4FjgFuMzMnspYVCIi0mZSTQRXx34IVunuW939AoIflYmIdGipDkMdP1R0OhUXFzNr1iyuuOKK2sHtOppUE0GpmX3TzO4EMLOhwIbMhSUiobRqDjwwAor7Bs+r5jS7iOy/VBPBfwMnAJfFpncCjzReXUSkhVbNgT/dCOUbAQ+e/3TjfieDxGGo33//fU455RTGjRvHuHHjeOONNxoss2fPHq688kpGjhzJ2LFjWbhwIRAMYf21r32Ns88+m8MPP5xbbrmldplf//rXHHHEEYwfP55rrrmmdqjr/Px88vLy6NOnD926dduv95IpqQ5DfZy7jzOz5QDu/pmZdcx3JCId05+/Dx+tbnz+pqVQvbd+WWUFPHsDlM5OvszAkfDVe5rcbOIw1AceeCAvvfQSubm5/O1vf+Oyyy4jcbSCRx55BDNj9erVrF+/nokTJ/LOO+8AsGLFCpYvX0737t0ZNmwYU6dOJRKJ8KMf/Yhly5bRq1cvzjzzTEaPHg3A9773PQCmTJnSZJztKdVEUBkbRdQhGGIa0FVDIpI+iUmgufJWqqys5IYbbmDFihVEIpHaBj7ea6+9xtSpUwE48sgj+dKXvlRbb8KECfTp0weAo48+mg8++IBt27Zx2mmn1fY6vvGNbyRdb0eVaiJ4CPgjcKCZ3Q1cDNzR9CIiInGa+ebOAyNih4US9BkCV85PWxgPPPAABx10ECtXriQajZKbm9ui5bt37177OpWhqjuDVG9e/zhwC8GvibcS3LJSl4+KSPpMuBNy8uqX5eQF5WlUXl7OoEGDyMrK4ne/+x3V1dUN6pxyyim1Q0e/8847fPjhhwwb1vhw2MceeyyLFi3is88+o6qqiqeffjqtMWdak4nAzA6oeQCfAE8Avwc+jpWJiKTHqEvgvIeCHgAWPJ/3UFCeRt/5zneYPXs2o0ePZv369fTs2TNpnWg0ysiRI5kyZQqzZs2q1xNIdMghh3Dbbbcxfvx4TjrpJAoLC2sPH3UGzQ1D/Q+C8wKWZLa7+2GZCqwxGoZapPMI0zDUu3btIj8/n6qqKi666CKuuuoqLrroonaJJa3DULv7oWmMTUSkyyouLubll19mz549TJw4kQsvvLC9Q0pZqieLMbOvAScT9BD+n7s/k6mgREQ6m/vvv7+9Q2i1lE4Wm9l/A9cBq4E1wHVmph+UiYh0Aan2CM4EjvLYCQUzmw2szVhUIiLSZlIdYuJdYGjc9JBYmYiIdHKp9gh6AW+b2RKCcwTjgRIzew7A3c/PUHwiIpJhqfYI7gS+CvwAKAbOiZX9NPZowMxyzWyJma00s7Vm9sMkdf7VzNaZ2Soz+4uZfalV70JEpBGRSIQxY8YwevToRgeZS5fi4mLMjHffrTtg8l//9V+YWYPxjJoya9as2kHr9qdOqprtEcTGGCp29zNauO69wJnuvsvMcoDXzOzP7v5mXJ3lQJG7f2Fm3wb+A+i4IzOJSEbNf28+Dy57kI92f8TAngOZNm4akw+bvF/rzMvLY8WKFQAsWLCAW2+9lUWLFqUh2uRGjhzJH/7wB+64IxiF56mnnmL48OEZ2146NNsjcPdqIGpmLfqZnAd2xSZzYg9PqLPQ3b+ITb4JDG7JNkSk65j/3nyK3yhm6+6tOM7W3VspfqOY+e+lb5yhzz//nH79+gHBD8AmTJjAuHHjGDlyJM8++ywAu3fvZvLkyYwePZoRI0bw5JNPAlBaWsppp53GMcccw6RJk9i6dWvSbVx44YW16/r73/9Onz59GDBgQO38J554gpEjRzJixAhmzJhRWz5z5szaYaxff/312vKysjK+/vWvc+yxx3LsscfWm5cuqZ4j2AWsNrOXgN01he5+Y1MLxXoTpcCXgUfc/a0mql8N/LmR9VwLXAswdOjQZFVEpIO7d8m9rP90faPzV5WtYl90X72yPdV7uPP1O5n7ztykyxx5wJHMGD8j6bwaFRUVjBkzhj179rB161b++te/ApCbm8sf//hHevfuzbZt2zj++OM5//zzefHFFzn44IOZPz9IQOXl5VRWVjJ16lSeffZZCgoKePLJJ7n99tv5zW9+02B7vXv3ZsiQIaxZs4Znn32WKVOmMHPmTAC2bNnCjBkzKC0tpV+/fkycOJFnnnmG4447jh/84AeUlpbSp08fzjjjDMaOHQvAtGnT+O53v8vJJ5/Mhx9+yKRJk3j77bebfM8tlWoimBd7tEisNzHGzPoCfzSzEe6+JrGemX0TKAJOa2Q9jwGPQTDEREvjEJGOLzEJNFeeqvhDQ4sXL+af//mfWbNmDe7ObbfdxquvvkpWVhabN2/m448/ZuTIkdx8883MmDGDc889l1NOOYU1a9awZs0azjrrLACqq6sZNGhQo9u89NJL+cMf/sCCBQv4y1/+UpsIli5dyumnn05BQQEAl19+Oa+++ipAvfIpU6bUDmP98ssvs27dutp1f/755+zatYt0SikRuPtsM8sDhrp7i29R6e47zGwhcDbBD9JqmdlXgNuB09w9vQOPi0iH0dw394lzJ7J1d8PDLYN6DmLm2TPTEsMJJ5zAtm3bKCsr44UXXqCsrIzS0lJycnIoLCxkz549HHHEESxbtowXXniBO+64gwkTJnDRRRcxfPhwFi9enNJ2zj33XKZPn05RURG9e/fer5ij0Shvvvlmi4fLbolUf1l8HrACeDE2Pabm0tEmlimI9QSIJZGzgPUJdcYCvwDOd/dPWhq8iHQd08ZNIzdSv7HLjeQybdy0tG1j/fr1VFdX079/f8rLyznwwAPJyclh4cKFfPDBB0Bw+KZHjx5885vfZPr06Sxbtoxhw4ZRVlZWmwgqKytZu7bx39T26NGDe++9l9tvv71e+fjx41m0aBHbtm2jurqaJ554gtNOO43jjjuORYsWsX37diorK3nqqbpR/idOnMjPfvaz2uma3k06pXpoqJjgtwOvALj7CjNrbuTRQcDs2HmCLGCOuz9vZncBJe7+HHAfkA88ZWYAH+o3CSLhVHN1ULqvGqo5RwDg7syePZtIJMLll1/Oeeedx8iRIykqKuLII48EYPXq1UyfPp2srCxycnL4+c9/Trdu3Zg7dy433ngj5eXlVFVVcdNNNzV5NdCll17aoGzQoEHcc889nHHGGbg7kydP5oILLgCCS09POOEE+vbtWxsvwEMPPcT111/PqFGjqKqq4tRTT+XRRx/dr32SqMlhqGsrmb3p7seb2XJ3HxsrW+Xuo9IaTQo0DLVI5xGmYag7krQOQx1nrZn9ExAxs8OBG4HM/SpDRETaTKq/LJ4KDCf4kdjvgXLgpgzFJCIibajJHoGZ5RIMP/1lgiGoT3D3zn+nZhFpM+5O7BygtIFUDvcnaq5HMJvg+v7VBGMNdd47L4hIm8vNzWX79u2tapyk5dyd7du3t/hS0+bOERzt7iMBzOzXwJJWxiciITR48GA2bdpEWVlZe4cSGrm5uQwe3LLReppLBJU1L9y9St07EWmJnJwcDj1Utz7v6JpLBKPN7PPYawPyYtNGMK7c/v1kTkRE2l2TicDdI20ViIiItI9ULx8VEZEuSolARCTklAhEREJOiUBEJOSUCEREQk6JQEQk5JQIRERCTolARCTklAhEREJOiUBEJOSUCEREQk6JQEQk5JQIRERCTolARCTklAhEREIuY4nAzHLNbImZrTSztWb2wyR1upvZk2b2rpm9ZWaFmYpHRESSy2SPYC9wpruPBsYAZ5vZ8Ql1rgY+c/cvAw8A92YwHhERSSJjicADu2KTObGHJ1S7AJgdez0XmGC6MbKISJvK6DkCM4uY2QrgE+Ald38rocohwEYAd68CyoH+SdZzrZmVmFlJWVlZJkMWEQmdjCYCd6929zHAYGC8mY1o5Xoec/cidy8qKChIa4wiImHXJlcNufsOYCFwdsKszcAQADPLBvoA29siJhERCWTyqqECM+sbe50HnAWsT6j2HPAvsdcXA39198TzCCIikkHZGVz3IGC2mUUIEs4cd3/ezO4CStz9OeDXwO/M7F3gU+DSDMYjIiJJZCwRuPsqYGyS8jvjXu8BvpGpGEREpHn6ZbGISMgpEYiIhJwSgYhIyCkRiIiEnBKBiEjIKRGIiIScEoGISMgpEYiIhJwSgYhIyCkRiIiEnBKBiEjIKRGIiIScEoGISMgpEYiIhJwSgYhIyCkRiIiEnBKBiEjIKRGIiIScEoGISMgpEYiIhJwSgYhIyCkRiIiEXMYSgZkNMbOFZrbOzNaa2bQkdfqY2Z/MbGWszpWZikdERJLLzuC6q4Cb3X2ZmfUCSs3sJXdfF1fnemCdu59nZgXABjN73N33ZTAuERGJk7Eegbtvdfdlsdc7gbeBQxKrAb3MzIB84FOCBCIiIm0kkz2CWmZWCIwF3kqY9TDwHLAF6AVMcfdoW8QkIiKBjJ8sNrN84GngJnf/PGH2JGAFcDAwBnjYzHonWce1ZlZiZiVlZWUZjlhEJFwymgjMLIcgCTzu7vOSVLkSmOeBd4F/AEcmVnL3x9y9yN2LCgoKMhmyiEjoZPKqIQN+Dbzt7v/ZSLUPgQmx+gcBw4D3MhWTiIg0lMlzBCcB3wJWm9mKWNltwFAAd38U+BEwy8xWAwbMcPdtGYxJREQSZCwRuPtrBI17U3W2ABMzFYOIiDRPvywWEQk5JQIRkZBTIhARCTklAhGRkFMiEBEJOSUCEZGQUyIQEQk5JQIRkZBrk9FHRUSk9Z5Zvpn7Fmxgy44KDu6bx/RJw7hwbOKo/q2nRCAi0oE9s3wzt85bTUVlNQCbd1Rw67zVAGlLBkoEIiJNqI46+6qi7KuKsre6uvb1vupo3euqKHsTpuvNr46yt968huvZ28hyGz/9gqjXj6mispr7FmxQIpDMynRXVDqWjvJ5u3vQICY0qpU1DWkTjW3Dhri6iYa4kXUkmVed2Arvh26RLLplxx6NvM7vnk23HnXTH2z/Ium6tuyoSFtcSgTSQFt0RSX93J2o1z1HPWjAognl7o57XfmfV2/h319Yz56q4OaAm3dUMOPpVXzw6W5OOGxAvUY16bfWxG+1KTXW1UnrVVanr9GNZFmDxrZ7dsPGNz83u1697g0a6Ujd6+wsujfWmDexjZrXwej8LVPy/mdsTtLoH9w3Lx27CQhJIugo33bak7tTWe1UVkepqnb2VUepikaprHIqo9F65T+ev642CdSoqKzmrufX0T07K2hMiG9cahqWWOMT18jE14tvfBpME5uOOk5dOXGNWk25JzR2DaZj7zcardt+w7hqlq1fr2Fcie+xJq6a143VS1xXTVzUW1fic/Dls/57a/I9xs1Pp71VUR546W88wN9Sqm9W9203viHNSWgIe+dm1zWUjTS2DRvirMYb4oT53ePWE8lqeaPbEU2fNKzeFzOAvJwI0ycNS9s2unwiSPe322i0phF1KquisUbUqaoOGtOaxrbuOa7hjSurrHaqosE3obp11TTU9Zdvcl2xZauiTderSkP39tPd+/j248v2ez0tYQZZZhixZ6srqyk3g6ys+GkjK6EeQFZWsnUFdYPp2Ly4enXrqnsO6hg5WYZhcdtJUs+MrCzq1Yt/Nuq2n5UFEB9PknpZyd9j7XRcPYiPK+E9xq3r355d2+j+/5+rj2vQ4CZryLOzrFXfdqV5Ne2UrhraD/ct2JD02+2Mp1fx9LJNdQ1xXIPbVEOcxsOFSeVEjJxI8B8r+A+WRU62kZMVfLvKyTays+r+E/aMZNUtU/M6K65e7D9pTly9nGTLxMqzI8b0p1aybde+BrEd2Ks7v716fG1jY1a/EaxrcOs3Son1DMOSNMpZifXUsLSJRxe9l/TQwyF98zj58AHtEJEkunDsIRk9itHlE0FjJ1T2VkXZvbeK7EgWeTkReuVm128gs7LoFmtMExvR7Ehw7DE7Yg0a1W6R2DLZWeRkGTn1GuKGjXGwTLCujvKt6o7JRyftit52zlEcObB3O0YmmdAWhx6kY+vyieDgvnmNftuZ952T2iGijq8tuqLScejzFvN0n2XKsKKiIi8pKUm5fuI5Agi+7fzkayP1hy4ioWFmpe5elGxel+8R6NuOiEjTunwigMyfaBER6cw0+qiISMgpEYiIhJwSgYhIyCkRiIiEnBKBiEjIdbrfEZhZGfBBKxcfAGxLYzhhoH0WLvq8O7b9+Xy+5O4FyWZ0ukSwP8yspLEfVEhy2mfhos+7Y8vU56NDQyIiIadEICIScmFLBI+1dwCdkPZZuOjz7tgy8vmE6hyBiIg0FLYegYiIJFAiEBEJOSUCEZGQUyKQVjOzo8zsUTOba2bfbu94JLPM7EIz+6WZPWlmE9s7HqljZoeZ2a/NbG5rlg9FIjCzs81sg5m9a2bfb+94OgIz+42ZfWJmaxLKU95X7v62u18HXALovp8dWJo+72fc/RrgOmBKJuMNkzR9Nu+5+9WtjqGrXzVkZhHgHeAsYBOwFLjM3de1a2DtzMxOBXYBv3X3EbGypPsKiAA/SVjFVe7+iZmdD3wb+J27/76t4peWSdfnHVvup8Dj7r6sjcLv0tL82cx194tbGkMY7lA2HnjX3d8DMLM/ABcAoU4E7v6qmRUmFCfdV+7+E+DcRtbzHPCcmc0HlAg6qHR83mZmwD3An5UE0idd/xf3RxgODR0CbIyb3hQrk4ZatK/M7HQze8jMfgG8kOngJO1a+n9jKvAV4GIzuy6TgUmL/y/2N7NHgbFmdmtLNxaGHoFkiLu/ArzSzmFIG3H3h4CH2jsOacjdtxOcu2mVMPQINgND4qYHx8qkIe2rcNHn3XG16WcThkSwFDjczA41s27ApcBz7RxTR6V9FS76vDuuNv1sunwicPcq4AZgAfA2MMfd17ZvVO3PzJ4AFgPDzGyTmV2tfdV16fPuuDrCZ9PlLx8VEZGmdfkegYiINE2JQEQk5JQIRERCTolARCTklAhEREJOiUBEJOSUCKTTM7MHzOymuOkFZvaruOmfmtm/NrH8XWb2lWa2UWxm30tS3tfMvtPEctVmtsLM1prZSjO72cz0/046FP1BSlfwOnAiQKyRHQAMj5t/IvBGYwu7+53u/nIrt90XaDQRABXuPsbdhxMMKfxV4Aet3JZIRigRSFfwBnBC7PVwYA2w08z6mVl34ChgmZkdY2aLzKw01msYBGBms8zs4tjrc8xsfazOQ2b2fNx2jjazV8zsPTO7MVZ2D/B/Yt/672sqyNiY8dcCN1ggYmb3mdlSM1tlZv+3pq6ZzTCz1bFexD2xsmtidVea2dNm1sPMepnZP8wsJ1and/y0SCo0+qh0eu6+xcyqzGwowbf/xQRD9p4AlAOrAQd+RjCme5mZTQHuBq6qWY+Z5QK/AE5193/Efvof70jgDKAXsMHMfg58Hxjh7mNSjPW92E1HDiS4L0a5ux8bS1ivm9n/xrZzAXCcu39hZgfEFp/n7r+Mxfpj4Gp3/5mZvQJMBp4hGJNmnrtXprTzRFAikK7jDYIkcCLwnwSJ4ESCRPA6MAwYAbwU3F+FCLA1YR1HAu+5+z9i008QfIOvMd/d9wJ7zewT4KD9jHkiMKqmNwL0AQ4nGPN/prt/AeDun8bmj4glgL5APsE4NAC/Am4hSARXAtfsZ1wSMkoE0lXUnCcYSXBoaCNwM/A5MBMwYK27n9DoGpq3N+51Na34/2Nmh8WW/SQW01R3X5BQZ1Iji88CLnT3lWZ2BXA6gLu/bmaFZnY6EHH3NY0sL5KUzhFIV/EGwS38PnX36ti36L4Eh4feADYABWZ2AoCZ5ZjZ8IR1bAAOi7ttYCo3aN9JcKioWWZWADwKPOzBaI8LgG/HHd8/wsx6Ai8BV5pZj1h5zaGhXsDWWP3LE1b/W4Jbhc5MJRaReEoE0lWsJrha6M2EsnJ33+bu+4CLgXvNbCWwgtiVRjXcvYLgCqAXzayUoJEvb2qjsTtDvW5maxo5WZxXc/ko8DLwv8APY/N+RXDv7GVmtobg/ES2u79IMPZ8iZmtAGouW/034C2C3s/6hO08DvQjOJwl0iIahlokjpnlu/suC04kPAL8zd0faO+4mhM7z3CBu3+rvWORzkfnCETqu8bM/gXoBiwn+JbeoZnZzwh+n3BOe8cinZN6BCIiIadzBCIiIadEICISckoEIiIhp0QgIhJySgQiIiGnRCAiEnL/HwUos8haWbdmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "decays = [0, 0.001, 0.01, 0.1]\n",
    "\n",
    "word1 = \"u n i t e d\"\n",
    "word1_title = '\"united\"'\n",
    "word2 = \"a l o n g\"\n",
    "word2_title = '\"along\"'\n",
    "word1_perps = []\n",
    "word2_perps = []\n",
    "perps = []\n",
    "\n",
    "for decay in decays:\n",
    "    dict_results = np.load(\"weight_decay_\" + str(decay) + \"_dict.npy\", allow_pickle=True).item()\n",
    "    word1_perp = np.exp(dict_results[word1])\n",
    "    word1_perps.append(word1_perp)\n",
    "    word2_perp = np.exp(dict_results[word2])\n",
    "    word2_perps.append(word2_perp)\n",
    "    \n",
    "total_losses = np.load(\"model_perplexities.npy\", allow_pickle=True).item()\n",
    "for decay in decays:\n",
    "    perps.append(total_losses[\"weight_decay_\" + str(decay)])\n",
    "\n",
    "#print(word_perps)\n",
    "plt.clf()\n",
    "plt.xscale(\"symlog\", linthresh=0.0015)\n",
    "\n",
    "plt.xticks(decays)\n",
    "plt.xlabel(\"Weight Decay\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "#plt.title(\"Class Specific Perplexities\")\n",
    "plt.plot(decays, word1_perps, marker='o', label = word1_title)\n",
    "plt.plot(decays, word2_perps, marker='o', label = word2_title)\n",
    "plt.plot(decays, perps, marker='o', label = \"Base Model\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('class_perp_2_notitle.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dropouts = [0, 0.1, 0.2]\n",
    "\n",
    "word1 = \"s c i e n t o l o g y\"\n",
    "word1_title = '\"scientology\"'\n",
    "word2 = \"s o m e\"\n",
    "word2_title = '\"some\"'\n",
    "word1_perps = []\n",
    "word2_perps = []\n",
    "perps = []\n",
    "\n",
    "for dropout in dropouts:\n",
    "    dict_results = np.load(\"dropout_\" + str(dropout) + \"_dict.npy\", allow_pickle=True).item()\n",
    "    word1_perp = np.exp(dict_results[word1])\n",
    "    word1_perps.append(word1_perp)\n",
    "    word2_perp = np.exp(dict_results[word2])\n",
    "    word2_perps.append(word2_perp)\n",
    "    \n",
    "total_losses = np.load(\"model_perplexities.npy\", allow_pickle=True).item()\n",
    "total_losses[\"dropout_0.1\"] = total_losses[\"weight_decay_0.01\"]\n",
    "for dropout in dropouts:\n",
    "    perps.append(total_losses[\"dropout_\" + str(dropout)])\n",
    "\n",
    "#print(word_perps)\n",
    "plt.clf()\n",
    "#plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "#plt.xscale(\"symlog\", linthresh=0.0015)\n",
    "plt.xticks(dropouts)\n",
    "#plt.ylim(3.5, 5)\n",
    "plt.xlabel(\"Dropout\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "#plt.title(\"Class Specific Perplexities\")\n",
    "plt.plot(dropouts, word1_perps, marker='o', label = word1_title)\n",
    "plt.plot(dropouts, word2_perps, marker='o', label = word2_title)\n",
    "plt.plot(dropouts, perps, marker='o', label = \"Base Model\")\n",
    "plt.legend()\n",
    "plt.savefig('class_perp_1_notitles.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
