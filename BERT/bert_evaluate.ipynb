{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates the performance of BERT models in varying classes to see if there are any class specific biases, and generates plots to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-processed dataset created in the fine-tuning notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "lm_datasets = DatasetDict()\n",
    "lm_datasets = lm_datasets.load_from_disk(\"processed_dataset\")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets['validation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom class based on the HuggingFace trainer is used to perform class-specific evaluations for all frequently occurring words.  The evaluation results on each of the fine-tuned models are saved as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "from collections import defaultdict\n",
    "\n",
    "class ClassEvaluator(Trainer):\n",
    "    \n",
    "    def eval_by_class(self, num_occurrences=10):\n",
    "        \n",
    "        model = self._wrap_model(self.model, training=False)\n",
    "        model.eval()\n",
    "        loss_by_class = defaultdict(list)\n",
    "        mean_loss_by_class = defaultdict(float)\n",
    "        eval_dataloader = self.get_eval_dataloader()\n",
    "        #eval_dataloader = DataLoader(lm_datasets[\"validation\"], shuffle=False)\n",
    "        prediction_loss_only = False\n",
    "        \n",
    "        for step, inputs in enumerate(eval_dataloader):\n",
    "            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=[])\n",
    "            labels = tokenizer.batch_decode(labels[0])\n",
    "            loss = float(loss.cpu().numpy())\n",
    "            for label in labels:\n",
    "                if label != -100:\n",
    "                    loss_by_class[label].append(loss)\n",
    "                    \n",
    "        for key, val in loss_by_class.items():\n",
    "            if len(val) >= num_occurrences:\n",
    "                mean_loss_by_class[key] = np.mean(val)\n",
    "        \n",
    "        return mean_loss_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"weight_decay_0.1\", \"weight_decay_0.001\", \"weight_decay_0.01\", \"weight_decay_0\", \"dropout_0\", \"dropout_0.2\", \"dropout_0.4\"]\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    model = model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        per_device_eval_batch_size = 1,\n",
    "        output_dir = \"eval_tmp\"\n",
    "    )\n",
    "\n",
    "    class_evaluator = ClassEvaluator(\n",
    "        model = model,\n",
    "        args = eval_args,\n",
    "        train_dataset = lm_datasets[\"train\"],\n",
    "        eval_dataset = lm_datasets[\"validation\"],\n",
    "        data_collator = data_collator\n",
    "    )\n",
    "\n",
    "    class_probs = class_evaluator.eval_by_class(25)\n",
    "    \n",
    "    print(model_name)\n",
    "    print(class_probs)\n",
    "    np.save(model_name + \"_dict.npy\", class_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words/classes with a high difference in perplexity across different weight decay and dropout settings and over 25 occurrences in the validation set are graphed.  These images are used in the final paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1 = np.load(\"weight_decay_0_dict.npy\", allow_pickle=True).item()\n",
    "dict_2 = np.load(\"weight_decay_0.1_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "l = []\n",
    "for key, _ in dict_1.items():\n",
    "    diff = dict_2[key] - dict_1[key]\n",
    "    l.append((diff, key))\n",
    "    \n",
    "l.sort()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1 = np.load(\"dropout_0_dict.npy\", allow_pickle=True).item()\n",
    "dict_2 = np.load(\"dropout_0.2_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "l = []\n",
    "for key, _ in dict_1.items():\n",
    "    diff = dict_2[key] - dict_1[key]\n",
    "    l.append((diff, key))\n",
    "    \n",
    "l.sort()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    model = model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    \n",
    "    eval_args = TrainingArguments(\n",
    "        per_device_eval_batch_size = 1,\n",
    "        output_dir = \"eval_tmp\"\n",
    "    )\n",
    "\n",
    "    class_evaluator = ClassEvaluator(\n",
    "        model = model,\n",
    "        args = eval_args,\n",
    "        train_dataset = lm_datasets[\"train\"],\n",
    "        eval_dataset = lm_datasets[\"validation\"],\n",
    "        data_collator = data_collator\n",
    "    )\n",
    "    eval_results = class_evaluator.evaluate()\n",
    "    print(model_name)\n",
    "    print(\"loss \" + str(eval_results))\n",
    "    losses[model_name] = np.exp(eval_results[\"eval_loss\"])\n",
    "    \n",
    "np.save(\"model_perplexities.npy\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "decays = [0, 0.001, 0.01, 0.1]\n",
    "\n",
    "word1 = \"u n i t e d\"\n",
    "word1_title = '\"united\"'\n",
    "word2 = \"a l o n g\"\n",
    "word2_title = '\"along\"'\n",
    "word1_perps = []\n",
    "word2_perps = []\n",
    "perps = []\n",
    "\n",
    "for decay in decays:\n",
    "    dict_results = np.load(\"weight_decay_\" + str(decay) + \"_dict.npy\", allow_pickle=True).item()\n",
    "    word1_perp = np.exp(dict_results[word1])\n",
    "    word1_perps.append(word1_perp)\n",
    "    word2_perp = np.exp(dict_results[word2])\n",
    "    word2_perps.append(word2_perp)\n",
    "    \n",
    "total_losses = np.load(\"model_perplexities.npy\", allow_pickle=True).item()\n",
    "for decay in decays:\n",
    "    perps.append(total_losses[\"weight_decay_\" + str(decay)])\n",
    "\n",
    "#print(word_perps)\n",
    "plt.clf()\n",
    "plt.xscale(\"symlog\", linthresh=0.0015)\n",
    "\n",
    "plt.xticks(decays)\n",
    "plt.xlabel(\"Weight Decay\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "#plt.title(\"Class Specific Perplexities\")\n",
    "plt.plot(decays, word1_perps, marker='o', label = word1_title)\n",
    "plt.plot(decays, word2_perps, marker='o', label = word2_title)\n",
    "plt.plot(decays, perps, marker='o', label = \"Base Model\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('class_perp_2_notitle.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dropouts = [0, 0.1, 0.2]\n",
    "\n",
    "word1 = \"s c i e n t o l o g y\"\n",
    "word1_title = '\"scientology\"'\n",
    "word2 = \"s o m e\"\n",
    "word2_title = '\"some\"'\n",
    "word1_perps = []\n",
    "word2_perps = []\n",
    "perps = []\n",
    "\n",
    "for dropout in dropouts:\n",
    "    dict_results = np.load(\"dropout_\" + str(dropout) + \"_dict.npy\", allow_pickle=True).item()\n",
    "    word1_perp = np.exp(dict_results[word1])\n",
    "    word1_perps.append(word1_perp)\n",
    "    word2_perp = np.exp(dict_results[word2])\n",
    "    word2_perps.append(word2_perp)\n",
    "    \n",
    "total_losses = np.load(\"model_perplexities.npy\", allow_pickle=True).item()\n",
    "total_losses[\"dropout_0.1\"] = total_losses[\"weight_decay_0.01\"]\n",
    "for dropout in dropouts:\n",
    "    perps.append(total_losses[\"dropout_\" + str(dropout)])\n",
    "\n",
    "#print(word_perps)\n",
    "plt.clf()\n",
    "#plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "#plt.xscale(\"symlog\", linthresh=0.0015)\n",
    "plt.xticks(dropouts)\n",
    "#plt.ylim(3.5, 5)\n",
    "plt.xlabel(\"Dropout\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "#plt.title(\"Class Specific Perplexities\")\n",
    "plt.plot(dropouts, word1_perps, marker='o', label = word1_title)\n",
    "plt.plot(dropouts, word2_perps, marker='o', label = word2_title)\n",
    "plt.plot(dropouts, perps, marker='o', label = \"Base Model\")\n",
    "plt.legend()\n",
    "plt.savefig('class_perp_1_notitles.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
