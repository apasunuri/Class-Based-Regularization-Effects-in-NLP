{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8y6mdIRRIEP"
   },
   "source": [
    "# Fine-tuning a masked language model (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGE60yo5RIES"
   },
   "source": [
    "This notebook will fine-tune BERT models from the pretrained settings for Masked Language Modelling on the Wikitext-V2 dataset with a variety of weight decay and dropout values. It was made from modifying the Huggingface tutorial with the same name found here: https://huggingface.co/course/chapter7/3?fw=tf.  In addition, it also pre-processes and saves the Wikitext dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B2htNaZBRIEW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "model = BertForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ahgc2ty_RIEW",
    "outputId": "2ab7d823-cad8-4af2-d924-0f8eb255e870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "bert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> BERT number of parameters: {round(bert_num_parameters)}M'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QZidjHl8RIEX"
   },
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "up6m2xuGRIEY"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jZBOB0atRIEY",
    "outputId": "99a36103-f57f-4027-9c3f-efd1258f914c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great idea.'\n",
      "'>>> This is a great day.'\n",
      "'>>> This is a great place.'\n",
      "'>>> This is a great time.'\n",
      "'>>> This is a great thing.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PJ5c14tURIEY",
    "outputId": "cb9d27be-b8d2-467d-f4a4-990bb30b78a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (C:\\Users\\Noah\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbc42ae69b04e1c9b986b305f8c3079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "wiki_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gOmItdE3RIEZ",
    "outputId": "09abf723-b62c-4174-9a2b-e1baa73d208c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Noah\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-67496e9edb819c55.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Text:  <unk> , short @-@ arc , high pressure xenon arc lamps have a color temperature closely <unk> noon sunlight and are used in solar simulators . That is , the <unk> of these lamps closely <unk> a heated black body <unk> that has a temperature close to that observed from the Sun . After they were first introduced during the 1940s , these lamps began replacing the shorter @-@ lived carbon arc lamps in movie <unk> . They are employed in typical 35mm , <unk> and the new digital <unk> film projection systems , automotive <unk> <unk> , high @-@ end \" tactical \" <unk> and other specialized uses . These arc lamps are an excellent source of short wavelength ultraviolet radiation and they have intense emissions in the near infrared , which is used in some night vision systems . \n",
      "'\n",
      "\n",
      "'>>> Text:  Field Marshal Antonio José de Sucre is portrayed as an intimate friend of the General . The historical Antonio José de Sucre , the Field Marshal of <unk> , had been the most trusted general of Simón Bolívar . García Márquez describes him as \" intelligent , methodical , shy , and superstitious \" . The Field Marshal is married to and has a daughter with Doña Mariana <unk> . In the first chapter of the novel , the General asks Sucre to succeed him as President of the Republic , but he rejects the idea . One of the reasons Sucre gives is that he wishes only to live his life for his family . Also at the beginning of the novel , Sucre 's death is foreshadowed . Sucre tells the General that he plans on celebrating the Feast of Saint Anthony in Quito with his family . When the General hears that Sucre has been assassinated in <unk> on his way back to Quito , he <unk> blood . \n",
      "'\n",
      "\n",
      "'>>> Text:  Norman Gary Finkelstein ( born December 8 , 1953 ) is an American political scientist , activist , professor , and author . His primary fields of research are the Israeli – Palestinian conflict and the politics of the Holocaust , an interest motivated by the experiences of his parents who were Jewish Holocaust survivors . He is a graduate of Binghamton University and received his <unk> in political science at Princeton University . He has held faculty positions at Brooklyn College , Rutgers University , Hunter College , New York University , and DePaul University where he was an assistant professor from 2001 to 2007 . \n",
      "'\n"
     ]
    }
   ],
   "source": [
    "sample = wiki_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Text: {row['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RqGIr4m4RIEZ",
    "outputId": "07f39224-d3a4-4e36-b5f1-7ca85467ef71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Noah\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-b6ab8a5dd0b34a33.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Noah\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-b53886597b1e56d5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Noah\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-ccc694868fe9065d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = wiki_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KzJhOypURIEa",
    "outputId": "9d6b04d5-70e2-42b8-a726-7e04f8b0e21d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CghIFLzBRIEa"
   },
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iD-6-SyVRIEa",
    "outputId": "10252406-5100-4b65-e63a-1942c9fcc0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Text 0 length: 2'\n",
      "'>>> Text 1 length: 9'\n",
      "'>>> Text 2 length: 2'\n"
     ]
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Text {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "grM5zHN1RIEa",
    "outputId": "abc13f5f-7091-4b4c-ac44-acafca2a141f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated texts length: 13'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated texts length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c1g3L3cyRIEb",
    "outputId": "f5d515ee-019d-4766-b6c1-40a50e2e7068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 13'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MpHy-JvCRIEb"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "r6Ba2KxWRIEb",
    "outputId": "23d353b2-f175-4950-fb11-baf4ecc55308"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Noah\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-99a172a97b170d7b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Noah\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-a4ed72012931aa98.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Noah\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-0a1ce786ded4725e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2405\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19247\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2089\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets.save_to_disk(\"processed_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2405\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19247\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2089\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "lm_datasets = DatasetDict()\n",
    "lm_datasets = lm_datasets.load_from_disk(\"processed_dataset\")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "6KWVWewbRIEc",
    "outputId": "2e3d831c-2c4a-4eec-d654-36aa0afa0387"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs parallel to the first game and follows the \" nameless \", a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" < unk > raven \". [SEP] [CLS] the game began development in 2010, carrying over a large portion of the work done on valkyria chronicles ii. while it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more < unk > for series newcomers. character designer < unk > honjou and composer hitoshi sakimoto both returned from previous entries, along with'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kEEjpERsRIEc"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19247\n"
     ]
    }
   ],
   "source": [
    "print(len(lm_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using amp half precision backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 03:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\.conda\\envs\\m4dl\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19247\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 10.49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14445' max='14445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14445/14445 1:00:05, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.822600</td>\n",
       "      <td>1.489798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.675100</td>\n",
       "      <td>1.488142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.606400</td>\n",
       "      <td>1.472458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.551800</td>\n",
       "      <td>1.464766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.506600</td>\n",
       "      <td>1.456560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.471500</td>\n",
       "      <td>1.437576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.439100</td>\n",
       "      <td>1.462479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.402000</td>\n",
       "      <td>1.418378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.361900</td>\n",
       "      <td>1.425511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.349700</td>\n",
       "      <td>1.410594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.325000</td>\n",
       "      <td>1.385131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.302400</td>\n",
       "      <td>1.405623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.281600</td>\n",
       "      <td>1.401037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.273200</td>\n",
       "      <td>1.371518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.259200</td>\n",
       "      <td>1.407637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weight_decay_0\\checkpoint-500\n",
      "Configuration saved in weight_decay_0\\checkpoint-500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-1000\n",
      "Configuration saved in weight_decay_0\\checkpoint-1000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-1500\n",
      "Configuration saved in weight_decay_0\\checkpoint-1500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-2000\n",
      "Configuration saved in weight_decay_0\\checkpoint-2000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-2500\n",
      "Configuration saved in weight_decay_0\\checkpoint-2500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-3000\n",
      "Configuration saved in weight_decay_0\\checkpoint-3000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-3500\n",
      "Configuration saved in weight_decay_0\\checkpoint-3500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-4000\n",
      "Configuration saved in weight_decay_0\\checkpoint-4000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-4500\n",
      "Configuration saved in weight_decay_0\\checkpoint-4500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-5000\n",
      "Configuration saved in weight_decay_0\\checkpoint-5000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-5500\n",
      "Configuration saved in weight_decay_0\\checkpoint-5500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-6000\n",
      "Configuration saved in weight_decay_0\\checkpoint-6000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-6500\n",
      "Configuration saved in weight_decay_0\\checkpoint-6500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-7000\n",
      "Configuration saved in weight_decay_0\\checkpoint-7000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-7500\n",
      "Configuration saved in weight_decay_0\\checkpoint-7500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-8000\n",
      "Configuration saved in weight_decay_0\\checkpoint-8000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-8000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-8500\n",
      "Configuration saved in weight_decay_0\\checkpoint-8500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-9000\n",
      "Configuration saved in weight_decay_0\\checkpoint-9000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-9000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-9500\n",
      "Configuration saved in weight_decay_0\\checkpoint-9500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-10000\n",
      "Configuration saved in weight_decay_0\\checkpoint-10000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-10000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-10500\n",
      "Configuration saved in weight_decay_0\\checkpoint-10500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-11000\n",
      "Configuration saved in weight_decay_0\\checkpoint-11000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-11000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-11500\n",
      "Configuration saved in weight_decay_0\\checkpoint-11500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-12000\n",
      "Configuration saved in weight_decay_0\\checkpoint-12000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-12000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-12500\n",
      "Configuration saved in weight_decay_0\\checkpoint-12500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-13000\n",
      "Configuration saved in weight_decay_0\\checkpoint-13000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-13500\n",
      "Configuration saved in weight_decay_0\\checkpoint-13500\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-13500\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0\\checkpoint-14000\n",
      "Configuration saved in weight_decay_0\\checkpoint-14000\\config.json\n",
      "Model weights saved in weight_decay_0\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weight_decay_0\n",
      "Configuration saved in weight_decay_0\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Weight decay: 0Perplexity:4.068027780016066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in weight_decay_0\\pytorch_model.bin\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 04:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\.conda\\envs\\m4dl\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19247\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 10.49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14445' max='14445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14445/14445 1:01:17, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.822800</td>\n",
       "      <td>1.489305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.674800</td>\n",
       "      <td>1.490565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.606200</td>\n",
       "      <td>1.474359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.550700</td>\n",
       "      <td>1.465934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.505100</td>\n",
       "      <td>1.456819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.464400</td>\n",
       "      <td>1.432261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.431200</td>\n",
       "      <td>1.456484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.393100</td>\n",
       "      <td>1.412177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.350800</td>\n",
       "      <td>1.416174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.337400</td>\n",
       "      <td>1.402703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.377929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.288000</td>\n",
       "      <td>1.395757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.266800</td>\n",
       "      <td>1.390308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.260300</td>\n",
       "      <td>1.361325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.245300</td>\n",
       "      <td>1.396276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-1000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-1000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-1500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-1500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-2000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-2000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-2500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-2500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-3000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-3000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-3500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-3500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-4000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-4000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-4500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-4500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-5000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-5000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-5500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-5500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-6000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-6000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-6500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-6500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-7000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-7000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-7500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-7500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-8000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-8000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-8000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-8500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-8500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-9000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-9000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-9000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-9500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-9500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-10000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-10000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-10000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-10500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-10500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-11000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-11000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-11000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-11500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-11500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-12000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-12000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-12000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-12500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-12500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-13000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-13000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-13500\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-13500\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-13500\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.1\\checkpoint-14000\n",
      "Configuration saved in weight_decay_0.1\\checkpoint-14000\\config.json\n",
      "Model weights saved in weight_decay_0.1\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weight_decay_0.1\n",
      "Configuration saved in weight_decay_0.1\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Weight decay: 0.1Perplexity:4.030469501050297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in weight_decay_0.1\\pytorch_model.bin\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 04:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\.conda\\envs\\m4dl\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19247\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 10.49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14445' max='14445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14445/14445 1:01:13, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.822900</td>\n",
       "      <td>1.489171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.675400</td>\n",
       "      <td>1.489993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.606300</td>\n",
       "      <td>1.472989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.552300</td>\n",
       "      <td>1.465808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.506700</td>\n",
       "      <td>1.458348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.471800</td>\n",
       "      <td>1.438198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.439000</td>\n",
       "      <td>1.462922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.401700</td>\n",
       "      <td>1.417821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.361400</td>\n",
       "      <td>1.426324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.349400</td>\n",
       "      <td>1.410945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.324700</td>\n",
       "      <td>1.384837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.301700</td>\n",
       "      <td>1.405252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.281000</td>\n",
       "      <td>1.400885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.272700</td>\n",
       "      <td>1.371833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.258400</td>\n",
       "      <td>1.406975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-1000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-1000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-1500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-1500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-2000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-2000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-2500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-2500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-3000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-3000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-3500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-3500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-4000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-4000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-4500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-4500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-5000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-5000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-5500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-5500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-6000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-6000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-6500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-6500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-7000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-7000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-7500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-7500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-8000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-8000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-8000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-8500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-8500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-9000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-9000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-9000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-9500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-9500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-10000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-10000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-10000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-10500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-10500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-11000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-11000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-11000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-11500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-11500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-12000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-12000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-12000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-12500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-12500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-13000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-13000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-13500\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-13500\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-13500\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.01\\checkpoint-14000\n",
      "Configuration saved in weight_decay_0.01\\checkpoint-14000\\config.json\n",
      "Model weights saved in weight_decay_0.01\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weight_decay_0.01\n",
      "Configuration saved in weight_decay_0.01\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Weight decay: 0.01Perplexity:4.067527345803425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in weight_decay_0.01\\pytorch_model.bin\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 04:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\.conda\\envs\\m4dl\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19247\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 10.49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14445' max='14445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14445/14445 1:01:19, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.822400</td>\n",
       "      <td>1.488343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.675300</td>\n",
       "      <td>1.487519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.606300</td>\n",
       "      <td>1.474778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.551700</td>\n",
       "      <td>1.462987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.507000</td>\n",
       "      <td>1.457924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.471700</td>\n",
       "      <td>1.437412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.439300</td>\n",
       "      <td>1.462606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.401900</td>\n",
       "      <td>1.416968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.362200</td>\n",
       "      <td>1.425924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.411779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.325300</td>\n",
       "      <td>1.385177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.302500</td>\n",
       "      <td>1.404978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.281900</td>\n",
       "      <td>1.400218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.273300</td>\n",
       "      <td>1.371601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.259200</td>\n",
       "      <td>1.407664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-1000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-1000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-1500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-1500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-2000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-2000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-2500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-2500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-3000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-3000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-3500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-3500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-4000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-4000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-4500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-4500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-5000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-5000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-5500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-5500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-6000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-6000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-6500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-6500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-7000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-7000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-7500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-7500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-8000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-8000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-8000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-8500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-8500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-9000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-9000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-9000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-9500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-9500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-10000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-10000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-10000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-10500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-10500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-11000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-11000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-11000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-11500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-11500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-12000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-12000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-12000\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-12500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-12500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-13000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-13000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-13500\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-13500\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-13500\\pytorch_model.bin\n",
      "Saving model checkpoint to weight_decay_0.001\\checkpoint-14000\n",
      "Configuration saved in weight_decay_0.001\\checkpoint-14000\\config.json\n",
      "Model weights saved in weight_decay_0.001\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weight_decay_0.001\n",
      "Configuration saved in weight_decay_0.001\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Weight decay: 0.001Perplexity:4.069863232278728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in weight_decay_0.001\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import math\n",
    "\n",
    "weight_decays = [0, 0.1, 0.01, 0.001]\n",
    "\n",
    "for decay_val in weight_decays:\n",
    "    \n",
    "    model_checkpoint = \"bert-base-uncased\"\n",
    "    model = BertForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "    batch_size = 20\n",
    "    # Show the training loss with every epoch\n",
    "    logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"weight_decay_\"+str(decay_val),\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=decay_val,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        push_to_hub=False,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "    )\n",
    "    \n",
    "    from transformers import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")    \n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Weight decay: \" + str(decay_val) +  \"Perplexity:\" + str(math.exp(eval_results['eval_loss'])))\n",
    "    \n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 04:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\.conda\\envs\\m4dl\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19247\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 10.49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14445' max='14445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14445/14445 1:00:14, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.687900</td>\n",
       "      <td>1.477764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.535900</td>\n",
       "      <td>1.478608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.462500</td>\n",
       "      <td>1.464348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.400600</td>\n",
       "      <td>1.454949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.348100</td>\n",
       "      <td>1.449947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.309200</td>\n",
       "      <td>1.431754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.269200</td>\n",
       "      <td>1.454478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.227400</td>\n",
       "      <td>1.414745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.182900</td>\n",
       "      <td>1.420637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.167400</td>\n",
       "      <td>1.417111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.139100</td>\n",
       "      <td>1.387102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.111300</td>\n",
       "      <td>1.408188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.086100</td>\n",
       "      <td>1.403935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.079000</td>\n",
       "      <td>1.374716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.062700</td>\n",
       "      <td>1.413729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dropout_0\\checkpoint-500\n",
      "Configuration saved in dropout_0\\checkpoint-500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-1000\n",
      "Configuration saved in dropout_0\\checkpoint-1000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-1500\n",
      "Configuration saved in dropout_0\\checkpoint-1500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-2000\n",
      "Configuration saved in dropout_0\\checkpoint-2000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-2500\n",
      "Configuration saved in dropout_0\\checkpoint-2500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-3000\n",
      "Configuration saved in dropout_0\\checkpoint-3000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-3500\n",
      "Configuration saved in dropout_0\\checkpoint-3500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-4000\n",
      "Configuration saved in dropout_0\\checkpoint-4000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-4500\n",
      "Configuration saved in dropout_0\\checkpoint-4500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-5000\n",
      "Configuration saved in dropout_0\\checkpoint-5000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-5500\n",
      "Configuration saved in dropout_0\\checkpoint-5500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-6000\n",
      "Configuration saved in dropout_0\\checkpoint-6000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-6500\n",
      "Configuration saved in dropout_0\\checkpoint-6500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-7000\n",
      "Configuration saved in dropout_0\\checkpoint-7000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-7500\n",
      "Configuration saved in dropout_0\\checkpoint-7500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-8000\n",
      "Configuration saved in dropout_0\\checkpoint-8000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-8000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-8500\n",
      "Configuration saved in dropout_0\\checkpoint-8500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-9000\n",
      "Configuration saved in dropout_0\\checkpoint-9000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-9000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-9500\n",
      "Configuration saved in dropout_0\\checkpoint-9500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-10000\n",
      "Configuration saved in dropout_0\\checkpoint-10000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-10000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-10500\n",
      "Configuration saved in dropout_0\\checkpoint-10500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-11000\n",
      "Configuration saved in dropout_0\\checkpoint-11000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-11000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-11500\n",
      "Configuration saved in dropout_0\\checkpoint-11500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-12000\n",
      "Configuration saved in dropout_0\\checkpoint-12000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-12000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-12500\n",
      "Configuration saved in dropout_0\\checkpoint-12500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-13000\n",
      "Configuration saved in dropout_0\\checkpoint-13000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0\\checkpoint-13500\n",
      "Configuration saved in dropout_0\\checkpoint-13500\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-13500\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0\\checkpoint-14000\n",
      "Configuration saved in dropout_0\\checkpoint-14000\\config.json\n",
      "Model weights saved in dropout_0\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dropout_0\n",
      "Configuration saved in dropout_0\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Dropout: 0Perplexity:4.097257217826608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dropout_0\\pytorch_model.bin\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 04:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\.conda\\envs\\m4dl\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19247\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 10.49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14445' max='14445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14445/14445 1:01:37, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.038000</td>\n",
       "      <td>1.518632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.859100</td>\n",
       "      <td>1.510691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.792100</td>\n",
       "      <td>1.501466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.736500</td>\n",
       "      <td>1.485995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.692700</td>\n",
       "      <td>1.477181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.659400</td>\n",
       "      <td>1.460625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.627900</td>\n",
       "      <td>1.483993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.594300</td>\n",
       "      <td>1.448256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.558100</td>\n",
       "      <td>1.443808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.546500</td>\n",
       "      <td>1.432633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.525500</td>\n",
       "      <td>1.403198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.498000</td>\n",
       "      <td>1.424380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.483000</td>\n",
       "      <td>1.421806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.474600</td>\n",
       "      <td>1.393224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.461000</td>\n",
       "      <td>1.423691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dropout_0.2\\checkpoint-500\n",
      "Configuration saved in dropout_0.2\\checkpoint-500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-1000\n",
      "Configuration saved in dropout_0.2\\checkpoint-1000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-1500\n",
      "Configuration saved in dropout_0.2\\checkpoint-1500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-2000\n",
      "Configuration saved in dropout_0.2\\checkpoint-2000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-2500\n",
      "Configuration saved in dropout_0.2\\checkpoint-2500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-3000\n",
      "Configuration saved in dropout_0.2\\checkpoint-3000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-3500\n",
      "Configuration saved in dropout_0.2\\checkpoint-3500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-4000\n",
      "Configuration saved in dropout_0.2\\checkpoint-4000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-4500\n",
      "Configuration saved in dropout_0.2\\checkpoint-4500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-5000\n",
      "Configuration saved in dropout_0.2\\checkpoint-5000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-5500\n",
      "Configuration saved in dropout_0.2\\checkpoint-5500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-6000\n",
      "Configuration saved in dropout_0.2\\checkpoint-6000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-6500\n",
      "Configuration saved in dropout_0.2\\checkpoint-6500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-7000\n",
      "Configuration saved in dropout_0.2\\checkpoint-7000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-7500\n",
      "Configuration saved in dropout_0.2\\checkpoint-7500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-8000\n",
      "Configuration saved in dropout_0.2\\checkpoint-8000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-8000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-8500\n",
      "Configuration saved in dropout_0.2\\checkpoint-8500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-9000\n",
      "Configuration saved in dropout_0.2\\checkpoint-9000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-9000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-9500\n",
      "Configuration saved in dropout_0.2\\checkpoint-9500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-10000\n",
      "Configuration saved in dropout_0.2\\checkpoint-10000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-10000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-10500\n",
      "Configuration saved in dropout_0.2\\checkpoint-10500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-11000\n",
      "Configuration saved in dropout_0.2\\checkpoint-11000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-11000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-11500\n",
      "Configuration saved in dropout_0.2\\checkpoint-11500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-12000\n",
      "Configuration saved in dropout_0.2\\checkpoint-12000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-12000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-12500\n",
      "Configuration saved in dropout_0.2\\checkpoint-12500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-13000\n",
      "Configuration saved in dropout_0.2\\checkpoint-13000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-13500\n",
      "Configuration saved in dropout_0.2\\checkpoint-13500\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-13500\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.2\\checkpoint-14000\n",
      "Configuration saved in dropout_0.2\\checkpoint-14000\\config.json\n",
      "Model weights saved in dropout_0.2\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dropout_0.2\n",
      "Configuration saved in dropout_0.2\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Dropout: 0.2Perplexity:4.135483218561814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dropout_0.2\\pytorch_model.bin\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Noah/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 04:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\.conda\\envs\\m4dl\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19247\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 10.49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14445' max='14445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14445/14445 1:01:31, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.051500</td>\n",
       "      <td>1.727323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.656200</td>\n",
       "      <td>1.689313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.500100</td>\n",
       "      <td>1.668526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.416500</td>\n",
       "      <td>1.649227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.353900</td>\n",
       "      <td>1.649804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.300800</td>\n",
       "      <td>1.625283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.257600</td>\n",
       "      <td>1.645586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.220100</td>\n",
       "      <td>1.603510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.178200</td>\n",
       "      <td>1.599943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.158000</td>\n",
       "      <td>1.592281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.132600</td>\n",
       "      <td>1.562306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>1.584769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.093000</td>\n",
       "      <td>1.576801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.078500</td>\n",
       "      <td>1.547037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.063300</td>\n",
       "      <td>1.583229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dropout_0.4\\checkpoint-500\n",
      "Configuration saved in dropout_0.4\\checkpoint-500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-1000\n",
      "Configuration saved in dropout_0.4\\checkpoint-1000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-1500\n",
      "Configuration saved in dropout_0.4\\checkpoint-1500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-2000\n",
      "Configuration saved in dropout_0.4\\checkpoint-2000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-2500\n",
      "Configuration saved in dropout_0.4\\checkpoint-2500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-3000\n",
      "Configuration saved in dropout_0.4\\checkpoint-3000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-3000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-3500\n",
      "Configuration saved in dropout_0.4\\checkpoint-3500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-4000\n",
      "Configuration saved in dropout_0.4\\checkpoint-4000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-4500\n",
      "Configuration saved in dropout_0.4\\checkpoint-4500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-5000\n",
      "Configuration saved in dropout_0.4\\checkpoint-5000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-5500\n",
      "Configuration saved in dropout_0.4\\checkpoint-5500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-6000\n",
      "Configuration saved in dropout_0.4\\checkpoint-6000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-6500\n",
      "Configuration saved in dropout_0.4\\checkpoint-6500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-7000\n",
      "Configuration saved in dropout_0.4\\checkpoint-7000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-7500\n",
      "Configuration saved in dropout_0.4\\checkpoint-7500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-8000\n",
      "Configuration saved in dropout_0.4\\checkpoint-8000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-8000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-8500\n",
      "Configuration saved in dropout_0.4\\checkpoint-8500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-9000\n",
      "Configuration saved in dropout_0.4\\checkpoint-9000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-9000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-9500\n",
      "Configuration saved in dropout_0.4\\checkpoint-9500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-10000\n",
      "Configuration saved in dropout_0.4\\checkpoint-10000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-10000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-10500\n",
      "Configuration saved in dropout_0.4\\checkpoint-10500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-11000\n",
      "Configuration saved in dropout_0.4\\checkpoint-11000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-11000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-11500\n",
      "Configuration saved in dropout_0.4\\checkpoint-11500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-12000\n",
      "Configuration saved in dropout_0.4\\checkpoint-12000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-12000\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-12500\n",
      "Configuration saved in dropout_0.4\\checkpoint-12500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-13000\n",
      "Configuration saved in dropout_0.4\\checkpoint-13000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-13500\n",
      "Configuration saved in dropout_0.4\\checkpoint-13500\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-13500\\pytorch_model.bin\n",
      "Saving model checkpoint to dropout_0.4\\checkpoint-14000\n",
      "Configuration saved in dropout_0.4\\checkpoint-14000\\config.json\n",
      "Model weights saved in dropout_0.4\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2089\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dropout_0.4\n",
      "Configuration saved in dropout_0.4\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Dropout: 0.4Perplexity:4.848238999974596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dropout_0.4\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, TrainingArguments\n",
    "import math\n",
    "dropouts = [0, 0.2, 0.4]\n",
    "\n",
    "for dropout_val in dropouts:\n",
    "    batch_size = 20\n",
    "    # Show the training loss with every epoch\n",
    "    logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "    dropout_config = BertConfig(hidden_dropout_prob = dropout_val, attention_probs_dropout_prob = dropout_val)\n",
    "    model_checkpoint = \"bert-base-uncased\"\n",
    "    model = BertForMaskedLM.from_pretrained(model_checkpoint, config=dropout_config)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"dropout_\"+str(dropout_val),\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        push_to_hub=False,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "    )\n",
    "    \n",
    "    from transformers import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")    \n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Dropout: \" + str(dropout_val) + \"Perplexity:\" + str(math.exp(eval_results['eval_loss'])))\n",
    "    \n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fine-tuning a masked language model (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
