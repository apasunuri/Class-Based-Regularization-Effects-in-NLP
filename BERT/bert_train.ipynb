{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8y6mdIRRIEP"
   },
   "source": [
    "# Fine-tuning a masked language model (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGE60yo5RIES"
   },
   "source": [
    "This notebook will fine-tune BERT models from the pretrained settings for Masked Language Modelling on the Wikitext-V2 dataset with a variety of weight decay and dropout values. It was made from modifying the Huggingface tutorial with the same name found here: https://huggingface.co/course/chapter7/3?fw=tf.  In addition, it also pre-processes and saves the Wikitext dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2htNaZBRIEW"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "model = BertForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ahgc2ty_RIEW",
    "outputId": "2ab7d823-cad8-4af2-d924-0f8eb255e870"
   },
   "outputs": [],
   "source": [
    "bert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> BERT number of parameters: {round(bert_num_parameters)}M'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZidjHl8RIEX"
   },
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "up6m2xuGRIEY"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZBOB0atRIEY",
    "outputId": "99a36103-f57f-4027-9c3f-efd1258f914c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJ5c14tURIEY",
    "outputId": "cb9d27be-b8d2-467d-f4a4-990bb30b78a0"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "wiki_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOmItdE3RIEZ",
    "outputId": "09abf723-b62c-4174-9a2b-e1baa73d208c"
   },
   "outputs": [],
   "source": [
    "sample = wiki_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Text: {row['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqGIr4m4RIEZ",
    "outputId": "07f39224-d3a4-4e36-b5f1-7ca85467ef71"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = wiki_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzJhOypURIEa",
    "outputId": "9d6b04d5-70e2-42b8-a726-7e04f8b0e21d"
   },
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CghIFLzBRIEa"
   },
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iD-6-SyVRIEa",
    "outputId": "10252406-5100-4b65-e63a-1942c9fcc0ff"
   },
   "outputs": [],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Text {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grM5zHN1RIEa",
    "outputId": "abc13f5f-7091-4b4c-ac44-acafca2a141f"
   },
   "outputs": [],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated texts length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1g3L3cyRIEb",
    "outputId": "f5d515ee-019d-4766-b6c1-40a50e2e7068"
   },
   "outputs": [],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpHy-JvCRIEb"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6Ba2KxWRIEb",
    "outputId": "23d353b2-f175-4950-fb11-baf4ecc55308"
   },
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets.save_to_disk(\"processed_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "lm_datasets = DatasetDict()\n",
    "lm_datasets = lm_datasets.load_from_disk(\"processed_dataset\")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KWVWewbRIEc",
    "outputId": "2e3d831c-2c4a-4eec-d654-36aa0afa0387"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEEjpERsRIEc"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lm_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import math\n",
    "\n",
    "weight_decays = [0, 0.1, 0.01, 0.001]\n",
    "\n",
    "for decay_val in weight_decays:\n",
    "    \n",
    "    model_checkpoint = \"bert-base-uncased\"\n",
    "    model = BertForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "    batch_size = 20\n",
    "    # Show the training loss with every epoch\n",
    "    logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"weight_decay_\"+str(decay_val),\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=decay_val,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        push_to_hub=False,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "    )\n",
    "    \n",
    "    from transformers import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")    \n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Weight decay: \" + str(decay_val) +  \"Perplexity:\" + str(math.exp(eval_results['eval_loss'])))\n",
    "    \n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TrainingArguments\n",
    "import math\n",
    "dropouts = [0, 0.2, 0.4]\n",
    "\n",
    "for dropout_val in dropouts:\n",
    "    batch_size = 20\n",
    "    # Show the training loss with every epoch\n",
    "    logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "    dropout_config = BertConfig(hidden_dropout_prob = dropout_val, attention_probs_dropout_prob = dropout_val)\n",
    "    model_checkpoint = \"bert-base-uncased\"\n",
    "    model = BertForMaskedLM.from_pretrained(model_checkpoint, config=dropout_config)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"dropout_\"+str(dropout_val),\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        push_to_hub=False,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "    )\n",
    "    \n",
    "    from transformers import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")    \n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\">>> Dropout: \" + str(dropout_val) + \"Perplexity:\" + str(math.exp(eval_results['eval_loss'])))\n",
    "    \n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fine-tuning a masked language model (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
