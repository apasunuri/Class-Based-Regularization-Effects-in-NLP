{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and LSTM Train\n",
    "This notebook will train an RNN and LSTM models on the Patent Classification and Bankings77 Dataset for different regularization values. It will load in the different raw data files and preprocess the data and then train each model for the regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X72IQnSlRHAK"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from dropconnect_tensorflow import DropConnect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSLvANoXTovY",
    "outputId": "7eb53c0b-37f2-4074-fe38-5641a4b51617",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 22\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZHks5-PmnDX"
   },
   "outputs": [],
   "source": [
    "def load_patent_data(data_path):\n",
    "    train = [json.loads(line) for line in open(f'{data_path}/train_data.txt', 'rb')]\n",
    "    val = [json.loads(line) for line in open(f'{data_path}/val_data.txt', 'rb')]\n",
    "    test = [json.loads(line) for line in open(f'{data_path}/test_data.txt', 'rb')]\n",
    "    data = train + val + test\n",
    "    text = [d['abstract'] for d in data]\n",
    "    labels = [d['label'] for d in data]\n",
    "    temp = list(zip(text, labels))\n",
    "    random.shuffle(temp)\n",
    "    text, labels = zip(*temp)\n",
    "    return text[:20000], labels[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhU0K8mQpanr"
   },
   "outputs": [],
   "source": [
    "def load_bankings_data(data_path):\n",
    "    train = pd.read_csv(f'{data_path}/train.csv')\n",
    "    test = pd.read_csv(f'{data_path}/test.csv')\n",
    "    data = pd.concat([train, test])\n",
    "    texts = data['text'].tolist()\n",
    "    labels = data['category'].tolist()\n",
    "    temp = list(zip(texts, labels))\n",
    "    random.shuffle(temp)\n",
    "    texts, labels = zip(*temp)\n",
    "    texts, labels = list(texts), list(labels)\n",
    "    return texts[:8000], labels[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3QjNSR6d8-s"
   },
   "outputs": [],
   "source": [
    "def load_article_data(data_path):\n",
    "    texts, labels = [], []\n",
    "    data = pd.read_csv(f'{data_path}/bbc-text.csv')\n",
    "    for row in data.iterrows():\n",
    "        texts.append(row[1].text)\n",
    "        labels.append(row[1].category)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfZ0DxS3o2gW"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    if os.path.basename(data_path) == 'patent_classification':\n",
    "        texts, labels = load_patent_data(data_path)\n",
    "    elif os.path.basename(data_path) == 'bankings_77':\n",
    "        texts, labels = load_bankings_data(data_path)\n",
    "    elif os.path.basename(data_path) == 'article_classification':\n",
    "        texts, labels = load_article_data(data_path)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxsOJHZTeB9a"
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_data(texts, labels):\n",
    "    special_symbols = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    processed_texts = []\n",
    "    for i, text in enumerate(texts):\n",
    "        processed_text = text.lower()\n",
    "        processed_text = special_symbols.sub(' ', processed_text)\n",
    "        processed_text = bad_symbols.sub('', processed_text)\n",
    "        processed_text = ' '.join(word for word in processed_text.split() if word not in stopwords_set)\n",
    "        processed_texts.append(processed_text)\n",
    "    return processed_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6DHbi68TJUZ"
   },
   "outputs": [],
   "source": [
    "def split_data(texts, labels, validation_size, test_size):\n",
    "    sequences, test_sequences, labels, test_labels = train_test_split(texts, labels, test_size=test_size, shuffle=False)\n",
    "    train_sequences, validation_sequences, train_labels, validation_labels = train_test_split(sequences, labels, test_size=validation_size, shuffle=False)\n",
    "    return [train_sequences, validation_sequences, test_sequences], [train_labels, validation_labels, test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyLe3eThlLGj"
   },
   "outputs": [],
   "source": [
    "def tokenize_data(X, y, labels, vocab_size, oov_token, padding_size, padding_type):\n",
    "    sequence_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "    sequence_tokenizer.fit_on_texts(X[0])\n",
    "    X_train = sequence_tokenizer.texts_to_sequences(X[0])\n",
    "    X_train = pad_sequences(X_train, maxlen=padding_size, padding=padding_type)\n",
    "    X_validation = sequence_tokenizer.texts_to_sequences(X[1])\n",
    "    X_validation = pad_sequences(X_validation, maxlen=padding_size, padding=padding_type)\n",
    "    X_test = sequence_tokenizer.texts_to_sequences(X[2])\n",
    "    X_test = pad_sequences(X_test, maxlen=padding_size, padding=padding_type)\n",
    "    label_tokenizer = LabelEncoder()\n",
    "    label_tokenizer.fit(labels)\n",
    "    y_train = label_tokenizer.transform(y[0])\n",
    "    y_validation = label_tokenizer.transform(y[1])\n",
    "    y_test = label_tokenizer.transform(y[2])\n",
    "    return [X_train, X_validation, X_test], [y_train, y_validation, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xbEZ0KaeHzM"
   },
   "outputs": [],
   "source": [
    "def get_model(vocab_size, hidden_layer_size, activation, num_labels, weight_decay, dropout_rate, dropconnect_rate, model_type):\n",
    "    if model_type == 'rnn':\n",
    "        sequential_layer = tf.keras.layers.SimpleRNN(hidden_layer_size, activation, kernel_regularizer=weight_decay, dropout=dropout_rate)\n",
    "    elif model_type == 'lstm':\n",
    "        sequential_layer = tf.keras.layers.LSTM(hidden_layer_size, activation, kernel_regularizer=weight_decay, dropout=dropout_rate)\n",
    "    sequential_layer = tf.keras.layers.Bidirectional(sequential_layer)\n",
    "    if dropconnect_rate > 0:\n",
    "        sequential_layer = DropConnect(sequential_layer, prob=dropconnect_rate)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, hidden_layer_size),\n",
    "        sequential_layer,\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_labels, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjbQT5ORenYc"
   },
   "outputs": [],
   "source": [
    "def run_model(X, y, model, loss, optimizer, metrics, epochs, batch_size, save_path, model_name):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    history = model.fit(X[0], y[0], batch_size=batch_size, epochs=epochs, validation_data=(X[1], y[1]))\n",
    "    model.evaluate(X[2], y[2], batch_size=batch_size)\n",
    "    model.save(f'{save_path}/{model_name}.h5')\n",
    "    with open(f'{save_path}/{model_name}.pickle', 'wb') as f:\n",
    "        pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH6WcJSuqCuQ"
   },
   "outputs": [],
   "source": [
    "save_path = './Saved Models'\n",
    "validation_size = 0.15\n",
    "test_size = 0.2\n",
    "vocab_size = 5000\n",
    "oov_token = '<OOV>'\n",
    "padding_size = 200\n",
    "padding_type = 'post'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patent Classification Dataset Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozhKelZpp1Sc"
   },
   "outputs": [],
   "source": [
    "data_path = './Data/patent_classification'\n",
    "texts, labels = load_data(data_path)\n",
    "processed_text, labels = get_preprocessed_data(texts, labels)\n",
    "X, y = split_data(processed_text, labels, validation_size, test_size)\n",
    "X, y = tokenize_data(X, y, labels, vocab_size, oov_token, padding_size, padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-ZDHUz7nX9R"
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 128\n",
    "num_labels = len(set(labels))\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "weight_decays_l1 = [0, 0.001, 0.01, 0.1]\n",
    "weight_decays_l2 = [0.001, 0.01, 0.1]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "dropconnect_rates = [0.1, 0.2, 0.3]\n",
    "dataset = os.path.basename(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS9iISjIeqEa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay_l1 in weight_decays_l1:\n",
    "    print(f'Training RNN on {dataset} with {weight_decay_l1} L1 Weight Decay...')\n",
    "    weight_decay = l1(weight_decay_l1) if weight_decay_l1 > 0 else None  \n",
    "    rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, weight_decay, 0, 0, 'rnn')\n",
    "    run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_l1_weight_decay_{weight_decay_l1}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8UOCXYfjg-o",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay_l2 in weight_decays_l2:\n",
    "    print(f'Training RNN on {dataset} with {weight_decay_l2} L2 Weight Decay...')\n",
    "    weight_decay = l2(weight_decay_l2) if weight_decay_l2 > 0 else None\n",
    "    rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, weight_decay, 0, 0, 'rnn')\n",
    "    run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_l2_weight_decay_{weight_decay_l2}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kh30z4OVwUqp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dropout_rate in dropout_rates:\n",
    "    print(f'Training RNN on {dataset} with {dropout_rate} Dropout...')\n",
    "    rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, None, dropout_rate, 0, 'rnn')\n",
    "    run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_dropout_{dropout_rate}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, None, 0.2, 0, 'rnn')\n",
    "run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_dropout_0.2_patent_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dZgKhtXuwNn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dropconnect_rate in dropconnect_rates:\n",
    "    print(f'Training RNN on {dataset} with {dropconnect_rate} Dropconnect...')\n",
    "    rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, None, 0, dropconnect_rate, 'rnn')\n",
    "    run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_dropconnect_{dropconnect_rate}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuWCoo3jqjyu"
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 128\n",
    "num_labels = len(set(labels))\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "weight_decays_l1 = [0, 0.001, 0.01, 0.1]\n",
    "weight_decays_l2 = [0.001, 0.01, 0.1]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "dropconnect_rates = [0.1, 0.2, 0.3]\n",
    "dataset = os.path.basename(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yak_C-Tbreq-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay_l1 in weight_decays_l1:\n",
    "    print(f'Training LSTM on {dataset} with {weight_decay_l1} L1 Weight Decay...')\n",
    "    weight_decay = l1(weight_decay_l1) if weight_decay_l1 > 0 else None\n",
    "    lstm_model = get_model(vocab_size, hidden_layer_size, 'tanh', num_labels, weight_decay, 0, 0, 'lstm')\n",
    "    run_model(X, y, lstm_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'lstm_l1_weight_decay_{weight_decay_l1}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSgYqpB1jIVR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay_l2 in weight_decays_l2:\n",
    "    print(f'Training LSTM on {dataset} with {weight_decay_l2} L2 Weight Decay...')\n",
    "    weight_decay = l2(weight_decay_l2) if weight_decay_l2 > 0 else None\n",
    "    lstm_model = get_model(vocab_size, hidden_layer_size, 'tanh', num_labels, weight_decay, 0, 0, 'lstm')\n",
    "    run_model(X, y, lstm_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'lstm_l2_weight_decay_{weight_decay_l2}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKmiJcJlv2rH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dropout_rate in dropout_rates:\n",
    "    print(f'Training LSTM on {dataset} with {dropout_rate} Dropout...')\n",
    "    lstm_model = get_model(vocab_size, hidden_layer_size, 'tanh', num_labels, None, dropout_rate, 0, 'lstm')\n",
    "    run_model(X, y, lstm_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'lstm_dropout_{dropout_rate}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwPbWWvNv0rD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dropconnect_rate in dropconnect_rates:\n",
    "    print(f'Training LSTM on {dataset} with {dropconnect_rate} Dropconnect...')\n",
    "    lstm_model = get_model(vocab_size, hidden_layer_size, 'tanh', num_labels, None, 0, dropconnect_rate, 'lstm')\n",
    "    run_model(X, y, lstm_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'lstm_dropconnect_{dropconnect_rate}_{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bankings77 Dataset Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4h1UQFyrH4S"
   },
   "outputs": [],
   "source": [
    "data_path = './Data/bankings_77'\n",
    "texts, labels = load_data(data_path)\n",
    "processed_text, labels = get_preprocessed_data(texts, labels)\n",
    "X, y = split_data(processed_text, labels, validation_size, test_size)\n",
    "X, y = tokenize_data(X, y, labels, vocab_size, oov_token, padding_size, padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vt5IIWgrIOV"
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 128\n",
    "num_labels = len(set(labels))\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "epochs = 60\n",
    "batch_size = 128\n",
    "weight_decays_l1 = [0, 0.001, 0.01, 0.1]\n",
    "weight_decays_l2 = [0.001, 0.01, 0.1]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "dropconnect_rates = [0.1, 0.2, 0.3]\n",
    "dataset = os.path.basename(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVfxbLfbrv0E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay_l1 in weight_decays_l1:\n",
    "    print(f'Training RNN on {dataset} with {weight_decay_l1} L1 Weight Decay...')\n",
    "    weight_decay = l1(weight_decay_l1) if weight_decay_l1 > 0 else None  \n",
    "    rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, weight_decay, 0, 0, 'rnn')\n",
    "    run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_l1_weight_decay_{weight_decay_l1}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbAnkZVArv0F",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay_l2 in weight_decays_l2:\n",
    "    print(f'Training RNN on {dataset} with {weight_decay_l2} L2 Weight Decay...')\n",
    "    weight_decay = l2(weight_decay_l2) if weight_decay_l2 > 0 else None\n",
    "    rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, weight_decay, 0, 0, 'rnn')\n",
    "    run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_l2_weight_decay_{weight_decay_l2}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxdRXXZtrv0G",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dropout_rate in dropout_rates:\n",
    "    print(f'Training RNN on {dataset} with {dropout_rate} Dropout...')\n",
    "    rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, None, dropout_rate, 0, 'rnn')\n",
    "    run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_dropout_{dropout_rate}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trHJm16FwarH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dropconnect_rate in dropconnect_rates:\n",
    "    print(f'Training RNN on {dataset} with {dropconnect_rate} Dropconnect...')\n",
    "    rnn_model = get_model(vocab_size, hidden_layer_size, 'relu', num_labels, None, 0, dropconnect_rate, 'rnn')\n",
    "    run_model(X, y, rnn_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'rnn_dropconnect_{dropconnect_rate}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brw_W9amr3Xf"
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 64\n",
    "num_labels = len(set(labels))\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "epochs = 80\n",
    "batch_size = 64\n",
    "weight_decays_l1 = [0, 0.001, 0.01, 0.1]\n",
    "weight_decays_l2 = [0.001, 0.01, 0.1]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "dropconnect_rates = [0.1, 0.2, 0.3]\n",
    "dataset = os.path.basename(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjE25pOVr3Xi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay_l1 in weight_decays_l1:\n",
    "    print(f'Training LSTM on {dataset} with {weight_decay_l1} L1 Weight Decay...')\n",
    "    weight_decay = l1(weight_decay_l1) if weight_decay_l1 > 0 else None\n",
    "    lstm_model = get_model(vocab_size, hidden_layer_size, 'tanh', num_labels, weight_decay, 0, 0, 'lstm')\n",
    "    run_model(X, y, lstm_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'lstm_l1_weight_decay_{weight_decay_l1}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CTRZ3dxr3Xj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay_l2 in weight_decays_l2:\n",
    "    print(f'Training LSTM on {dataset} with {weight_decay_l2} L2 Weight Decay...')\n",
    "    weight_decay = l2(weight_decay_l2) if weight_decay_l2 > 0 else None\n",
    "    lstm_model = get_model(vocab_size, hidden_layer_size, 'tanh', num_labels, weight_decay, 0, 0, 'lstm')\n",
    "    run_model(X, y, lstm_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'lstm_l2_weight_decay_{weight_decay_l2}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RszsBergr3Xk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dropout_rate in dropout_rates:\n",
    "    print(f'Training LSTM on {dataset} with {dropout_rate} Dropout...')\n",
    "    lstm_model = get_model(vocab_size, hidden_layer_size, 'tanh', num_labels, None, dropout_rate, 0, 'lstm')\n",
    "    run_model(X, y, lstm_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'lstm_dropout_{dropout_rate}_{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4cpSeyNwXNJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dropconnect_rate in dropconnect_rates:\n",
    "    print(f'Training LSTM on {dataset} with {dropconnect_rate} Dropconnect...')\n",
    "    lstm_model = get_model(vocab_size, hidden_layer_size, 'tanh', num_labels, None, 0, dropconnect_rate, 'lstm')\n",
    "    run_model(X, y, lstm_model, loss, optimizer, metrics, epochs, batch_size, save_path, f'lstm_dropconnect_{dropconnect_rate}_{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "The references and resources were used for developing the code.\n",
    "- https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "- https://towardsdatascience.com/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35\n",
    "- https://huggingface.co/docs/datasets/index"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
